{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 - Understanding and Predicting Property Maintenance Fines\n",
    "\n",
    "This assignment is based on a data challenge from the Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)). \n",
    "\n",
    "The Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)) and the Michigan Student Symposium for Interdisciplinary Statistical Sciences ([MSSISS](https://sites.lsa.umich.edu/mssiss/)) have partnered with the City of Detroit to help solve one of the most pressing problems facing Detroit - blight. [Blight violations](http://www.detroitmi.gov/How-Do-I/Report/Blight-Complaint-FAQs) are issued by the city to individuals who allow their properties to remain in a deteriorated condition. Every year, the city of Detroit issues millions of dollars in fines to residents and every year, many of these fines remain unpaid. Enforcing unpaid blight fines is a costly and tedious process, so the city wants to know: how can we increase blight ticket compliance?\n",
    "\n",
    "The first step in answering this question is understanding when and why a resident might fail to comply with a blight ticket. This is where predictive modeling comes in. For this assignment, your task is to predict whether a given blight ticket will be paid on time.\n",
    "\n",
    "All data for this assignment has been provided to us through the [Detroit Open Data Portal](https://data.detroitmi.gov/). **Only the data already included in your Coursera directory can be used for training the model for this assignment.** Nonetheless, we encourage you to look into data from other Detroit datasets to help inform feature creation and model selection. We recommend taking a look at the following related datasets:\n",
    "\n",
    "* [Building Permits](https://data.detroitmi.gov/Property-Parcels/Building-Permits/xw2a-a7tf)\n",
    "* [Trades Permits](https://data.detroitmi.gov/Property-Parcels/Trades-Permits/635b-dsgv)\n",
    "* [Improve Detroit: Submitted Issues](https://data.detroitmi.gov/Government/Improve-Detroit-Submitted-Issues/fwz3-w3yn)\n",
    "* [DPD: Citizen Complaints](https://data.detroitmi.gov/Public-Safety/DPD-Citizen-Complaints-2016/kahe-efs3)\n",
    "* [Parcel Map](https://data.detroitmi.gov/Property-Parcels/Parcel-Map/fxkw-udwf)\n",
    "\n",
    "___\n",
    "\n",
    "We provide you with two data files for use in training and validating your models: train.csv and test.csv. Each row in these two files corresponds to a single blight ticket, and includes information about when, why, and to whom each ticket was issued. The target variable is compliance, which is True if the ticket was paid early, on time, or within one month of the hearing data, False if the ticket was paid after the hearing date or not at all, and Null if the violator was found not responsible. Compliance, as well as a handful of other variables that will not be available at test-time, are only included in train.csv.\n",
    "\n",
    "Note: All tickets where the violators were found not responsible are not considered during evaluation. They are included in the training set as an additional source of data for visualization, and to enable unsupervised and semi-supervised approaches. However, they are not included in the test set.\n",
    "\n",
    "<br>\n",
    "\n",
    "**File descriptions** (Use only this data for training your model!)\n",
    "\n",
    "    train.csv - the training set (all tickets issued 2004-2011)\n",
    "    test.csv - the test set (all tickets issued 2012-2016)\n",
    "    addresses.csv & latlons.csv - mapping from ticket id to addresses, and from addresses to lat/lon coordinates. \n",
    "     Note: misspelled addresses may be incorrectly geolocated.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Data fields**\n",
    "\n",
    "train.csv & test.csv\n",
    "\n",
    "    ticket_id - unique identifier for tickets\n",
    "    agency_name - Agency that issued the ticket\n",
    "    inspector_name - Name of inspector that issued the ticket\n",
    "    violator_name - Name of the person/organization that the ticket was issued to\n",
    "    violation_street_number, violation_street_name, violation_zip_code - Address where the violation occurred\n",
    "    mailing_address_str_number, mailing_address_str_name, city, state, zip_code, non_us_str_code, country - Mailing address of the violator\n",
    "    ticket_issued_date - Date and time the ticket was issued\n",
    "    hearing_date - Date and time the violator's hearing was scheduled\n",
    "    violation_code, violation_description - Type of violation\n",
    "    disposition - Judgment and judgement type\n",
    "    fine_amount - Violation fine amount, excluding fees\n",
    "    admin_fee - $20 fee assigned to responsible judgments\n",
    "state_fee - $10 fee assigned to responsible judgments\n",
    "    late_fee - 10% fee assigned to responsible judgments\n",
    "    discount_amount - discount applied, if any\n",
    "    clean_up_cost - DPW clean-up or graffiti removal cost\n",
    "    judgment_amount - Sum of all fines and fees\n",
    "    grafitti_status - Flag for graffiti violations\n",
    "    \n",
    "train.csv only\n",
    "\n",
    "    payment_amount - Amount paid, if any\n",
    "    payment_date - Date payment was made, if it was received\n",
    "    payment_status - Current payment status as of Feb 1 2017\n",
    "    balance_due - Fines and fees still owed\n",
    "    collection_status - Flag for payments in collections\n",
    "    compliance [target variable for prediction] \n",
    "     Null = Not responsible\n",
    "     0 = Responsible, non-compliant\n",
    "     1 = Responsible, compliant\n",
    "    compliance_detail - More information on why each ticket was marked compliant or non-compliant\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Your predictions will be given as the probability that the corresponding blight ticket will be paid on time.\n",
    "\n",
    "The evaluation metric for this assignment is the Area Under the ROC Curve (AUC). \n",
    "\n",
    "Your grade will be based on the AUC score computed for your classifier. A model which with an AUROC of 0.7 passes this assignment, over 0.75 will recieve full points.\n",
    "___\n",
    "\n",
    "For this assignment, create a function that trains a model to predict blight ticket compliance in Detroit using `train.csv`. Using this model, return a series of length 61001 with the data being the probability that each corresponding ticket from `test.csv` will be paid, and the index being the ticket_id.\n",
    "\n",
    "Example:\n",
    "\n",
    "    ticket_id\n",
    "       284932    0.531842\n",
    "       285362    0.401958\n",
    "       285361    0.105928\n",
    "       285338    0.018572\n",
    "                 ...\n",
    "       376499    0.208567\n",
    "       376500    0.818759\n",
    "       369851    0.018528\n",
    "       Name: compliance, dtype: float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (250306, 34) , test set: (61001, 27) , latlons: (121769, 3) , address: (311307, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read files.\n",
    "# The training data is indeed not encoded as UTF-8 so note the encoding parameter.\n",
    "train = pd.read_csv('train.csv', encoding='cp1252')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "latlons = pd.read_csv('latlons.csv')\n",
    "address = pd.read_csv('addresses.csv')\n",
    "\n",
    "print('training set:', train.shape,',', 'test set:', test.shape,',', 'latlons:',latlons.shape,',', 'address:',address.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Leakage\n",
    "\n",
    "We need to remove all data just prior to the event of interest, in our case, 'payment_amount','balance_due', 'payment_due', 'payment_status', etc. These columns will cause our model too optimistic, which leads to a overfitting problem since this kind of information will not be given with unseen data in practice. Also without knowing the result of the status of tickets will be paid on time, there is no way we can know them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['payment_amount', 'balance_due', 'payment_date', 'payment_status', 'collection_status', 'compliance_detail']\n"
     ]
    }
   ],
   "source": [
    "# In order to avoid either data leakage or too many missing values, remove some columns does not exist \n",
    "# in test dataset except for the target variable 'compliance'.\n",
    "elimi_col = [col for col in train.columns if(col not in test.columns)][:-1]\n",
    "print(elimi_col)\n",
    "\n",
    "train = train.drop(elimi_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22056</td>\n",
       "      <td>2900 tyler, Detroit MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27586</td>\n",
       "      <td>4311 central, Detroit MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22062</td>\n",
       "      <td>1449 longfellow, Detroit MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22084</td>\n",
       "      <td>1441 longfellow, Detroit MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22093</td>\n",
       "      <td>2449 churchill, Detroit MI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                      address\n",
       "0      22056       2900 tyler, Detroit MI\n",
       "1      27586     4311 central, Detroit MI\n",
       "2      22062  1449 longfellow, Detroit MI\n",
       "3      22084  1441 longfellow, Detroit MI\n",
       "4      22093   2449 churchill, Detroit MI"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4300 rosa parks blvd, Detroit MI 48208</td>\n",
       "      <td>42.346169</td>\n",
       "      <td>-83.079962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14512 sussex, Detroit MI</td>\n",
       "      <td>42.394657</td>\n",
       "      <td>-83.194265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3456 garland, Detroit MI</td>\n",
       "      <td>42.373779</td>\n",
       "      <td>-82.986228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5787 wayburn, Detroit MI</td>\n",
       "      <td>42.403342</td>\n",
       "      <td>-82.957805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5766 haverhill, Detroit MI</td>\n",
       "      <td>42.407255</td>\n",
       "      <td>-82.946295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  address        lat        lon\n",
       "0  4300 rosa parks blvd, Detroit MI 48208  42.346169 -83.079962\n",
       "1                14512 sussex, Detroit MI  42.394657 -83.194265\n",
       "2                3456 garland, Detroit MI  42.373779 -82.986228\n",
       "3                5787 wayburn, Detroit MI  42.403342 -82.957805\n",
       "4              5766 haverhill, Detroit MI  42.407255 -82.946295"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latlons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By just looking at the first sight, it appears that we can merge address and latlons dataframes by the common column \"address\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>address</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22056</td>\n",
       "      <td>2900 tyler, Detroit MI</td>\n",
       "      <td>42.390729</td>\n",
       "      <td>-83.124268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77242</td>\n",
       "      <td>2900 tyler, Detroit MI</td>\n",
       "      <td>42.390729</td>\n",
       "      <td>-83.124268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77243</td>\n",
       "      <td>2900 tyler, Detroit MI</td>\n",
       "      <td>42.390729</td>\n",
       "      <td>-83.124268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103945</td>\n",
       "      <td>2900 tyler, Detroit MI</td>\n",
       "      <td>42.390729</td>\n",
       "      <td>-83.124268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138219</td>\n",
       "      <td>2900 tyler, Detroit MI</td>\n",
       "      <td>42.390729</td>\n",
       "      <td>-83.124268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                 address        lat        lon\n",
       "0      22056  2900 tyler, Detroit MI  42.390729 -83.124268\n",
       "1      77242  2900 tyler, Detroit MI  42.390729 -83.124268\n",
       "2      77243  2900 tyler, Detroit MI  42.390729 -83.124268\n",
       "3     103945  2900 tyler, Detroit MI  42.390729 -83.124268\n",
       "4     138219  2900 tyler, Detroit MI  42.390729 -83.124268"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loca = address.merge(latlons, how = 'outer', left_on = 'address', right_on = 'address')\n",
    "loca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a same property can have multiple ticket_id (tickets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>agency_name</th>\n",
       "      <th>inspector_name</th>\n",
       "      <th>violator_name</th>\n",
       "      <th>violation_street_number</th>\n",
       "      <th>violation_street_name</th>\n",
       "      <th>violation_zip_code</th>\n",
       "      <th>mailing_address_str_number</th>\n",
       "      <th>mailing_address_str_name</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>state_fee</th>\n",
       "      <th>late_fee</th>\n",
       "      <th>discount_amount</th>\n",
       "      <th>clean_up_cost</th>\n",
       "      <th>judgment_amount</th>\n",
       "      <th>grafitti_status</th>\n",
       "      <th>compliance</th>\n",
       "      <th>address</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22056</td>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>Sims, Martinzie</td>\n",
       "      <td>INVESTMENT INC., MIDWEST MORTGAGE</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>TYLER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>S. WICKER</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2900 tyler, Detroit MI</td>\n",
       "      <td>42.390729</td>\n",
       "      <td>-83.124268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27586</td>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>Williams, Darrin</td>\n",
       "      <td>Michigan, Covenant House</td>\n",
       "      <td>4311.0</td>\n",
       "      <td>CENTRAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2959.0</td>\n",
       "      <td>Martin Luther King</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4311 central, Detroit MI</td>\n",
       "      <td>42.326937</td>\n",
       "      <td>-83.135118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22062</td>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>Sims, Martinzie</td>\n",
       "      <td>SANDERS, DERRON</td>\n",
       "      <td>1449.0</td>\n",
       "      <td>LONGFELLOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23658.0</td>\n",
       "      <td>P.O. BOX</td>\n",
       "      <td>DETROIT</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1449 longfellow, Detroit MI</td>\n",
       "      <td>42.380516</td>\n",
       "      <td>-83.096069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22084</td>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>Sims, Martinzie</td>\n",
       "      <td>MOROSI, MIKE</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>LONGFELLOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>ST. CLAIR</td>\n",
       "      <td>DETROIT</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1441 longfellow, Detroit MI</td>\n",
       "      <td>42.380570</td>\n",
       "      <td>-83.095919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22093</td>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>Sims, Martinzie</td>\n",
       "      <td>NATHANIEL, NEAL</td>\n",
       "      <td>2449.0</td>\n",
       "      <td>CHURCHILL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7449.0</td>\n",
       "      <td>CHURCHILL</td>\n",
       "      <td>DETROIT</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2449 churchill, Detroit MI</td>\n",
       "      <td>42.145257</td>\n",
       "      <td>-83.208233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                                     agency_name  \\\n",
       "0      22056  Buildings, Safety Engineering & Env Department   \n",
       "1      27586  Buildings, Safety Engineering & Env Department   \n",
       "2      22062  Buildings, Safety Engineering & Env Department   \n",
       "3      22084  Buildings, Safety Engineering & Env Department   \n",
       "4      22093  Buildings, Safety Engineering & Env Department   \n",
       "\n",
       "     inspector_name                      violator_name  \\\n",
       "0   Sims, Martinzie  INVESTMENT INC., MIDWEST MORTGAGE   \n",
       "1  Williams, Darrin           Michigan, Covenant House   \n",
       "2   Sims, Martinzie                    SANDERS, DERRON   \n",
       "3   Sims, Martinzie                       MOROSI, MIKE   \n",
       "4   Sims, Martinzie                    NATHANIEL, NEAL   \n",
       "\n",
       "   violation_street_number violation_street_name  violation_zip_code  \\\n",
       "0                   2900.0                 TYLER                 NaN   \n",
       "1                   4311.0               CENTRAL                 NaN   \n",
       "2                   1449.0            LONGFELLOW                 NaN   \n",
       "3                   1441.0            LONGFELLOW                 NaN   \n",
       "4                   2449.0             CHURCHILL                 NaN   \n",
       "\n",
       "   mailing_address_str_number mailing_address_str_name     city    ...      \\\n",
       "0                         3.0                S. WICKER  CHICAGO    ...       \n",
       "1                      2959.0       Martin Luther King  Detroit    ...       \n",
       "2                     23658.0                 P.O. BOX  DETROIT    ...       \n",
       "3                         5.0                ST. CLAIR  DETROIT    ...       \n",
       "4                      7449.0                CHURCHILL  DETROIT    ...       \n",
       "\n",
       "  state_fee late_fee discount_amount clean_up_cost judgment_amount  \\\n",
       "0      10.0     25.0             0.0           0.0           305.0   \n",
       "1      10.0     75.0             0.0           0.0           855.0   \n",
       "2       0.0      0.0             0.0           0.0             0.0   \n",
       "3       0.0      0.0             0.0           0.0             0.0   \n",
       "4       0.0      0.0             0.0           0.0             0.0   \n",
       "\n",
       "  grafitti_status compliance                      address        lat  \\\n",
       "0             NaN        0.0       2900 tyler, Detroit MI  42.390729   \n",
       "1             NaN        1.0     4311 central, Detroit MI  42.326937   \n",
       "2             NaN        NaN  1449 longfellow, Detroit MI  42.380516   \n",
       "3             NaN        NaN  1441 longfellow, Detroit MI  42.380570   \n",
       "4             NaN        NaN   2449 churchill, Detroit MI  42.145257   \n",
       "\n",
       "         lon  \n",
       "0 -83.124268  \n",
       "1 -83.135118  \n",
       "2 -83.096069  \n",
       "3 -83.095919  \n",
       "4 -83.208233  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(loca, how = 'inner', left_on = 'ticket_id', right_on = 'ticket_id')\n",
    "test = test.merge(loca, how = 'inner',left_on = 'ticket_id', right_on = 'ticket_id')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (250306, 31)\n",
      "test set: (61001, 30)\n"
     ]
    }
   ],
   "source": [
    "print('training set:', train.shape)\n",
    "print(\"test set:\",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing & Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "violator_name                     34\n",
       "violation_zip_code            250306\n",
       "mailing_address_str_number      3602\n",
       "mailing_address_str_name           4\n",
       "state                             93\n",
       "zip_code                           1\n",
       "non_us_str_code               250303\n",
       "hearing_date                   12491\n",
       "fine_amount                        1\n",
       "grafitti_status               250305\n",
       "compliance                     90426\n",
       "lat                                3\n",
       "lon                                3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()[train.isnull().sum() >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "violator_name                    28\n",
       "violation_zip_code            36977\n",
       "mailing_address_str_number     1014\n",
       "mailing_address_str_name          3\n",
       "state                           331\n",
       "zip_code                          3\n",
       "non_us_str_code               61001\n",
       "hearing_date                   2197\n",
       "fine_amount                       0\n",
       "grafitti_status               58780\n",
       "lat                               5\n",
       "lon                               5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()[train.isnull().sum() >= 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the above, three columns 'violation_zipcode','non_us_str_code','grafitti_status' have too many missing values. Therefore, remove these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Violation Zip Code, Non Us Str Code, Grafitti status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.drop(['violation_zip_code','non_us_str_code','grafitti_status'], axis=1, inplace=True)\n",
    "test.drop(['violation_zip_code','non_us_str_code','grafitti_status'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Number of NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For the convenience for counting the number of nulls in each row, create the new column 'num_null'\n",
    "train['num_null'] = train.isnull().sum(axis=1)\n",
    "test['num_null'] = test.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12 Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training lat and lon: 2 Missing values in test lat and lon: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>agency_name</th>\n",
       "      <th>inspector_name</th>\n",
       "      <th>violator_name</th>\n",
       "      <th>violation_street_number</th>\n",
       "      <th>violation_street_name</th>\n",
       "      <th>mailing_address_str_number</th>\n",
       "      <th>mailing_address_str_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>...</th>\n",
       "      <th>state_fee</th>\n",
       "      <th>late_fee</th>\n",
       "      <th>discount_amount</th>\n",
       "      <th>clean_up_cost</th>\n",
       "      <th>judgment_amount</th>\n",
       "      <th>compliance</th>\n",
       "      <th>address</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>num_null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191722</th>\n",
       "      <td>223598</td>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>O'Neal, Claude</td>\n",
       "      <td>HJADJANTONI, MICHAEL</td>\n",
       "      <td>445.0</td>\n",
       "      <td>FORDYCE</td>\n",
       "      <td>437.0</td>\n",
       "      <td>FOREST</td>\n",
       "      <td>DETROIT</td>\n",
       "      <td>MI</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>445 fordyce, Detroit MI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245981</th>\n",
       "      <td>280256</td>\n",
       "      <td>Department of Public Works</td>\n",
       "      <td>McCants, Angela</td>\n",
       "      <td>HOLDINGS LLC, FORDYCE</td>\n",
       "      <td>8300.0</td>\n",
       "      <td>FORDYCE</td>\n",
       "      <td>151.0</td>\n",
       "      <td>POTOMIC</td>\n",
       "      <td>ROCHESTER HILLS</td>\n",
       "      <td>MI</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8300 fordyce, Detroit MI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ticket_id                                     agency_name  \\\n",
       "191722     223598  Buildings, Safety Engineering & Env Department   \n",
       "245981     280256                      Department of Public Works   \n",
       "\n",
       "         inspector_name          violator_name  violation_street_number  \\\n",
       "191722   O'Neal, Claude   HJADJANTONI, MICHAEL                    445.0   \n",
       "245981  McCants, Angela  HOLDINGS LLC, FORDYCE                   8300.0   \n",
       "\n",
       "       violation_street_name  mailing_address_str_number  \\\n",
       "191722               FORDYCE                       437.0   \n",
       "245981               FORDYCE                       151.0   \n",
       "\n",
       "       mailing_address_str_name             city state    ...    state_fee  \\\n",
       "191722                   FOREST          DETROIT    MI    ...         10.0   \n",
       "245981                  POTOMIC  ROCHESTER HILLS    MI    ...          0.0   \n",
       "\n",
       "       late_fee discount_amount clean_up_cost judgment_amount compliance  \\\n",
       "191722     25.0             0.0           0.0           305.0        0.0   \n",
       "245981      0.0             0.0           0.0             0.0        NaN   \n",
       "\n",
       "                         address  lat  lon  num_null  \n",
       "191722   445 fordyce, Detroit MI  NaN  NaN         2  \n",
       "245981  8300 fordyce, Detroit MI  NaN  NaN         3  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training set lat,lon:\n",
    "# Since there are some missing values found in both training and test set, \n",
    "# we need to manually find the lat and lon coordinates from google map by searching the given addresses\n",
    "\n",
    "# 20424 bramford, Detroit MI, (42.446558,-83.022996)\n",
    "train.iloc[65578,-3] = 42.446558\n",
    "train.iloc[65578,-2] = -83.022996\n",
    "\n",
    "# Fail to find the lat and lon coordinates for the ticket_id 223598 and 280256 from google map \n",
    "# since the rest of two other places are showing too wide area (google map cannot pinpoint where it is).\n",
    "\n",
    "# Test Set lat,lon:\n",
    "\n",
    "# 20424 bramford, Detroit MI, (42.446558,-83.022996)\n",
    "test.iloc[20459,-3] = 42.446558\n",
    "test.iloc[20459,-2] = -83.022996\n",
    "\n",
    "# 8325 joy rd, Detroit MI 48204, (42.359024, -83.150799)\n",
    "test.iloc[28350,-3] = 42.359024 \n",
    "test.iloc[28350,-2] = -83.150799\n",
    "\n",
    "# 1201 elijah mccoy dr, Detroit MI 48208, (42.358760, -83.080382)\n",
    "test.iloc[28416,-3] = 42.358760\n",
    "test.iloc[28416,-2] = -83.080382\n",
    "\n",
    "# 12038 prairie, Detroit MI 482O4, (42.376943, -83.143206)\n",
    "test.iloc[31925,-3] = 42.376943\n",
    "test.iloc[31925,-2] = -83.143206\n",
    "\n",
    "# 6200 16th st, Detroit MI 48208 (42.360104, -83.095872)\n",
    "test.iloc[55400,-3] = 42.360104\n",
    "test.iloc[55400,-2] = -83.095872\n",
    "\n",
    "# Check how many null values are in train and test\n",
    "print('Missing values in training lat and lon:',len(train[train.lon.isnull()]), 'Missing values in test lat and lon:',len(test[test.lon.isnull()]))\n",
    "\n",
    "# Check the table.\n",
    "train[train.lon.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As you can see, there are still two missing values which cannot be found in google map; therefore, needed to be filled with\n",
    "# 0.0 lat and lon (or some distinguishable number from other lat, lon but I decided to choose 0.0). \n",
    "# Except for these two values, all the values do not have any problem so leave the rows instead of dropping the rows.\n",
    "train.lat[train.lat.isnull()] = 0.0\n",
    "train.lon[train.lon.isnull()] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we converted most of violation addresses to lat and lon coordinate, 'violation_street_number','violation_street_name','address' variables are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13 Violation Street Number , Violation Street Name, Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.drop(['violation_street_number','violation_street_name','address'], axis = 1, inplace = True)\n",
    "test.drop(['violation_street_number','violation_street_name','address'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.14 Country / State / City / New Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mailing_address_str_number</th>\n",
       "      <th>mailing_address_str_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>country</th>\n",
       "      <th>ticket_issued_date</th>\n",
       "      <th>hearing_date</th>\n",
       "      <th>violation_code</th>\n",
       "      <th>violation_description</th>\n",
       "      <th>disposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183911</th>\n",
       "      <td>NaN</td>\n",
       "      <td>P. O. BOX 26334</td>\n",
       "      <td>MANAMA HAHRAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2009-04-22 10:45:00</td>\n",
       "      <td>2009-05-28 10:30:00</td>\n",
       "      <td>22-2-88</td>\n",
       "      <td>Failure of owner to keep property, its sidewal...</td>\n",
       "      <td>Responsible by Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186126</th>\n",
       "      <td>3212.0</td>\n",
       "      <td>MCCARRON CRES</td>\n",
       "      <td>MISS, ONTARIO, CANADA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L5N 3H5</td>\n",
       "      <td>USA</td>\n",
       "      <td>2009-05-08 10:40:00</td>\n",
       "      <td>2009-06-18 13:30:00</td>\n",
       "      <td>22-2-88</td>\n",
       "      <td>Failure of owner to keep property, its sidewal...</td>\n",
       "      <td>Responsible by Admission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188915</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FOXWOOD CHALKY ROAD</td>\n",
       "      <td>STOCKBURG KENT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2009-06-04 13:30:00</td>\n",
       "      <td>2009-10-29 10:30:00</td>\n",
       "      <td>9-1-104</td>\n",
       "      <td>Excessive weeds or plant growth one- or two-fa...</td>\n",
       "      <td>Responsible by Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190375</th>\n",
       "      <td>1.0</td>\n",
       "      <td>PARTIGLIANO</td>\n",
       "      <td>LUCCA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55067</td>\n",
       "      <td>USA</td>\n",
       "      <td>2009-06-23 13:30:00</td>\n",
       "      <td>2009-08-05 13:30:00</td>\n",
       "      <td>9-1-105</td>\n",
       "      <td>Rodent harborage one-or two-family dwelling or...</td>\n",
       "      <td>Responsible by Admission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191245</th>\n",
       "      <td>29.0</td>\n",
       "      <td>CHERRY  TREE  CT</td>\n",
       "      <td>CHARLESTON LONDON, ENGLAND</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SE 770X</td>\n",
       "      <td>USA</td>\n",
       "      <td>2009-07-07 09:50:00</td>\n",
       "      <td>2009-08-18 13:30:00</td>\n",
       "      <td>9-1-104</td>\n",
       "      <td>Excessive weeds or plant growth one- or two-fa...</td>\n",
       "      <td>Responsible by Admission</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mailing_address_str_number mailing_address_str_name  \\\n",
       "183911                         NaN          P. O. BOX 26334   \n",
       "186126                      3212.0            MCCARRON CRES   \n",
       "188915                         NaN      FOXWOOD CHALKY ROAD   \n",
       "190375                         1.0              PARTIGLIANO   \n",
       "191245                        29.0         CHERRY  TREE  CT   \n",
       "\n",
       "                              city state zip_code country  \\\n",
       "183911              MANAMA HAHRAIN   NaN        0     USA   \n",
       "186126       MISS, ONTARIO, CANADA   NaN  L5N 3H5     USA   \n",
       "188915              STOCKBURG KENT   NaN        0     USA   \n",
       "190375                       LUCCA   NaN    55067     USA   \n",
       "191245  CHARLESTON LONDON, ENGLAND   NaN  SE 770X     USA   \n",
       "\n",
       "         ticket_issued_date         hearing_date violation_code  \\\n",
       "183911  2009-04-22 10:45:00  2009-05-28 10:30:00        22-2-88   \n",
       "186126  2009-05-08 10:40:00  2009-06-18 13:30:00        22-2-88   \n",
       "188915  2009-06-04 13:30:00  2009-10-29 10:30:00        9-1-104   \n",
       "190375  2009-06-23 13:30:00  2009-08-05 13:30:00        9-1-105   \n",
       "191245  2009-07-07 09:50:00  2009-08-18 13:30:00        9-1-104   \n",
       "\n",
       "                                    violation_description  \\\n",
       "183911  Failure of owner to keep property, its sidewal...   \n",
       "186126  Failure of owner to keep property, its sidewal...   \n",
       "188915  Excessive weeds or plant growth one- or two-fa...   \n",
       "190375  Rodent harborage one-or two-family dwelling or...   \n",
       "191245  Excessive weeds or plant growth one- or two-fa...   \n",
       "\n",
       "                     disposition  \n",
       "183911    Responsible by Default  \n",
       "186126  Responsible by Admission  \n",
       "188915    Responsible by Default  \n",
       "190375  Responsible by Admission  \n",
       "191245  Responsible by Admission  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see the country and city columns, there are some discrepancies between city and country. \n",
    "# ex) on ticket number 218570, city is Ontario, Canada but the country denotes USA.\n",
    "train[train.state.isnull()].iloc[:,4:15].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mailing_address_str_name</th>\n",
       "      <th>city</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183911</th>\n",
       "      <td>P. O. BOX 26334</td>\n",
       "      <td>MANAMA HAHRAIN</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186126</th>\n",
       "      <td>MCCARRON CRES</td>\n",
       "      <td>MISS, ONTARIO, CANADA</td>\n",
       "      <td>L5N 3H5</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188915</th>\n",
       "      <td>FOXWOOD CHALKY ROAD</td>\n",
       "      <td>STOCKBURG KENT</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190375</th>\n",
       "      <td>PARTIGLIANO</td>\n",
       "      <td>LUCCA</td>\n",
       "      <td>55067</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191245</th>\n",
       "      <td>CHERRY  TREE  CT</td>\n",
       "      <td>CHARLESTON LONDON, ENGLAND</td>\n",
       "      <td>SE 770X</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193547</th>\n",
       "      <td>THORN  STREET</td>\n",
       "      <td>Woodville, United Kingdom</td>\n",
       "      <td>Deli-7DN</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194701</th>\n",
       "      <td>TOOLEY</td>\n",
       "      <td>LONDON SE12ET</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203041</th>\n",
       "      <td>BRIARWOOD AVE</td>\n",
       "      <td>TORONTO  ONTARIO, CANADA</td>\n",
       "      <td>M9W6C9</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203042</th>\n",
       "      <td>BRIARWOOD AVE</td>\n",
       "      <td>TORONTO  ONTARIO, CANADA</td>\n",
       "      <td>M9W6C9</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203043</th>\n",
       "      <td>BRIARWOOD AVE</td>\n",
       "      <td>TORONTO  ONTARIO, CANADA</td>\n",
       "      <td>M9W6C9</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216567</th>\n",
       "      <td>TAIMOR</td>\n",
       "      <td>ST. FATIMA SQ. HELIOPOLIS, CAIRO, EGYPT</td>\n",
       "      <td>11361</td>\n",
       "      <td>Egyp</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216568</th>\n",
       "      <td>TAIMOR ST</td>\n",
       "      <td>ST. FATIMA SQ. HELIOPOLIS, CAIRO, EGYPT</td>\n",
       "      <td>11361</td>\n",
       "      <td>Egyp</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216927</th>\n",
       "      <td>ELMONT  DRIVE</td>\n",
       "      <td>CALGARY, ALBERTA, CANADA</td>\n",
       "      <td>T3H-4X8</td>\n",
       "      <td>Cana</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217529</th>\n",
       "      <td>ELMONT DR.</td>\n",
       "      <td>CALGARY, ALBERTA, CANADA</td>\n",
       "      <td>T3H-4X8</td>\n",
       "      <td>Cana</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220371</th>\n",
       "      <td>ROBINA TOWN CENTER DR</td>\n",
       "      <td>ROBINA  QLD</td>\n",
       "      <td>4226</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220407</th>\n",
       "      <td>LAUREN MEWS HAYLING ISLAND</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220533</th>\n",
       "      <td>MILLER  BALLARAT</td>\n",
       "      <td>VICTORIA</td>\n",
       "      <td>3350</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223243</th>\n",
       "      <td>FOXWOOD, CHALKY RD.</td>\n",
       "      <td>STOCKBURY,  KENT</td>\n",
       "      <td>ME9,7QR UK</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223334</th>\n",
       "      <td>RUE DES LILAS</td>\n",
       "      <td>ALFORTEVILLE, FRANCE</td>\n",
       "      <td>94140</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223335</th>\n",
       "      <td>RUE DES LILAS</td>\n",
       "      <td>ALFORTEVILLE, FRANCE</td>\n",
       "      <td>94140</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224720</th>\n",
       "      <td>ELMONT DR.</td>\n",
       "      <td>CALVARY ALBERTA</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225113</th>\n",
       "      <td>BLK 213 BATILE BATOK, STE 012</td>\n",
       "      <td>SINGAPORE</td>\n",
       "      <td>650213</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225794</th>\n",
       "      <td>P.O. BOX</td>\n",
       "      <td>BET ZAIT, ISRAEL</td>\n",
       "      <td>90815</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225795</th>\n",
       "      <td>P.O. BOX</td>\n",
       "      <td>BET ZAIT, ISRAEL</td>\n",
       "      <td>90815</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225796</th>\n",
       "      <td>P.O. BOX</td>\n",
       "      <td>BET ZAIT, ISRAEL</td>\n",
       "      <td>90815</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227074</th>\n",
       "      <td>GLENNIE PLACE</td>\n",
       "      <td>QUEANBEYAN, NSW</td>\n",
       "      <td>2620</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227161</th>\n",
       "      <td>PINE LODGE, LOWOOD, ARMATHWAIT</td>\n",
       "      <td>CARLISLE CUMBRIAN</td>\n",
       "      <td>49</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227736</th>\n",
       "      <td>CLUNE TERRACE</td>\n",
       "      <td>NEWTONMORE</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227737</th>\n",
       "      <td>CLUNE TERRACE</td>\n",
       "      <td>NEWTONMORE</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227964</th>\n",
       "      <td>ELMONT, DR.</td>\n",
       "      <td>CALGARY ALBERTA</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244324</th>\n",
       "      <td>ALBANY  HWY</td>\n",
       "      <td>MT. BARKER</td>\n",
       "      <td>6324</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244407</th>\n",
       "      <td>BUKIT BATOK CRESENT</td>\n",
       "      <td>WCEGA TOWER 03-74</td>\n",
       "      <td>658065</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244547</th>\n",
       "      <td>CORNER OF GRADUATE CRESCENT</td>\n",
       "      <td>BELIZE CITY</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244660</th>\n",
       "      <td>180,ON NUT S0135,SUKNHUMVIT</td>\n",
       "      <td>BANGKOK</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244661</th>\n",
       "      <td>180,ON NUT SOI  35  SUKHUMVIT</td>\n",
       "      <td>BONGKOK</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244799</th>\n",
       "      <td>MASLISANSKY ST</td>\n",
       "      <td>RAMOT JERUSALEM</td>\n",
       "      <td>97225</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244856</th>\n",
       "      <td>NAVAHO DR.</td>\n",
       "      <td>TORONTO</td>\n",
       "      <td>222</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244894</th>\n",
       "      <td>WHITEGLEN CRES</td>\n",
       "      <td>N. E. CALGARY</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244895</th>\n",
       "      <td>NAVAHO DR.</td>\n",
       "      <td>TORONTO</td>\n",
       "      <td>22</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245023</th>\n",
       "      <td>TURNER CLOSE CREWE</td>\n",
       "      <td>CHESHIRE</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245024</th>\n",
       "      <td>TURNER CREWE</td>\n",
       "      <td>CHESHIRE</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245302</th>\n",
       "      <td>BERKLEY CRES NW</td>\n",
       "      <td>CALGARY,ALBERTA</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245316</th>\n",
       "      <td>ELMONT DR. SW</td>\n",
       "      <td>CALGARY</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245957</th>\n",
       "      <td>MAYNARD RD</td>\n",
       "      <td>WALTHAM</td>\n",
       "      <td>179</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245958</th>\n",
       "      <td>MAYNARD RD</td>\n",
       "      <td>WALTHAM</td>\n",
       "      <td>179</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246218</th>\n",
       "      <td>ABBOTS GARDENS</td>\n",
       "      <td>LODON ,N2OJE,UK</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247060</th>\n",
       "      <td>HAGALIL</td>\n",
       "      <td>GANIC TIKVA ,ISRAEL</td>\n",
       "      <td>55900</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247422</th>\n",
       "      <td>DE 'I INFANTE</td>\n",
       "      <td>WATERLOO</td>\n",
       "      <td>1401</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247515</th>\n",
       "      <td>RADFORD</td>\n",
       "      <td>ONTARIO</td>\n",
       "      <td>342</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247907</th>\n",
       "      <td>KNIGHT CLOSE</td>\n",
       "      <td>NEW SOUTH WALES</td>\n",
       "      <td>2448</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248034</th>\n",
       "      <td>SASAMEMINAMI</td>\n",
       "      <td>TADA CITY</td>\n",
       "      <td>3350035</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248257</th>\n",
       "      <td>HORSESHOE WAY  UNIT 133</td>\n",
       "      <td>RICHMOND BC</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248379</th>\n",
       "      <td>BUKIT BATOK CRESCENT</td>\n",
       "      <td>SINGAPORE</td>\n",
       "      <td>658065</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248967</th>\n",
       "      <td>TYNE ST</td>\n",
       "      <td>BOX HILL NORTH, VICTORIA AUSTRALIA</td>\n",
       "      <td>3129</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248968</th>\n",
       "      <td>TYNE ST</td>\n",
       "      <td>BOX HILL NORTH, VICTORIA AUSTRALIA</td>\n",
       "      <td>3129</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249009</th>\n",
       "      <td>MANOR FARM,SOUTH NEWTON</td>\n",
       "      <td>SALISBURY,WILT,SP2, OQD,</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249505</th>\n",
       "      <td>TENTH LIVERPOOL ST.</td>\n",
       "      <td>ROSE BAY</td>\n",
       "      <td>2029</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249762</th>\n",
       "      <td>LAUNCESTON PL</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>157</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249874</th>\n",
       "      <td>CLADE ST.</td>\n",
       "      <td>BEULAH SOUTH</td>\n",
       "      <td>5067</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250070</th>\n",
       "      <td>CRESCENT WCEGA TOWER</td>\n",
       "      <td>BUKIT BATOK</td>\n",
       "      <td>658065</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              mailing_address_str_name  \\\n",
       "183911                 P. O. BOX 26334   \n",
       "186126                   MCCARRON CRES   \n",
       "188915             FOXWOOD CHALKY ROAD   \n",
       "190375                     PARTIGLIANO   \n",
       "191245                CHERRY  TREE  CT   \n",
       "193547                   THORN  STREET   \n",
       "194701                          TOOLEY   \n",
       "203041                   BRIARWOOD AVE   \n",
       "203042                   BRIARWOOD AVE   \n",
       "203043                   BRIARWOOD AVE   \n",
       "216567                          TAIMOR   \n",
       "216568                       TAIMOR ST   \n",
       "216927                   ELMONT  DRIVE   \n",
       "217529                      ELMONT DR.   \n",
       "220371           ROBINA TOWN CENTER DR   \n",
       "220407      LAUREN MEWS HAYLING ISLAND   \n",
       "220533                MILLER  BALLARAT   \n",
       "223243             FOXWOOD, CHALKY RD.   \n",
       "223334                   RUE DES LILAS   \n",
       "223335                   RUE DES LILAS   \n",
       "224720                      ELMONT DR.   \n",
       "225113   BLK 213 BATILE BATOK, STE 012   \n",
       "225794                        P.O. BOX   \n",
       "225795                        P.O. BOX   \n",
       "225796                        P.O. BOX   \n",
       "227074                   GLENNIE PLACE   \n",
       "227161  PINE LODGE, LOWOOD, ARMATHWAIT   \n",
       "227736                   CLUNE TERRACE   \n",
       "227737                   CLUNE TERRACE   \n",
       "227964                     ELMONT, DR.   \n",
       "...                                ...   \n",
       "244324                     ALBANY  HWY   \n",
       "244407             BUKIT BATOK CRESENT   \n",
       "244547     CORNER OF GRADUATE CRESCENT   \n",
       "244660     180,ON NUT S0135,SUKNHUMVIT   \n",
       "244661   180,ON NUT SOI  35  SUKHUMVIT   \n",
       "244799                  MASLISANSKY ST   \n",
       "244856                      NAVAHO DR.   \n",
       "244894                  WHITEGLEN CRES   \n",
       "244895                      NAVAHO DR.   \n",
       "245023              TURNER CLOSE CREWE   \n",
       "245024                    TURNER CREWE   \n",
       "245302                 BERKLEY CRES NW   \n",
       "245316                   ELMONT DR. SW   \n",
       "245957                      MAYNARD RD   \n",
       "245958                      MAYNARD RD   \n",
       "246218                  ABBOTS GARDENS   \n",
       "247060                         HAGALIL   \n",
       "247422                   DE 'I INFANTE   \n",
       "247515                         RADFORD   \n",
       "247907                    KNIGHT CLOSE   \n",
       "248034                    SASAMEMINAMI   \n",
       "248257         HORSESHOE WAY  UNIT 133   \n",
       "248379            BUKIT BATOK CRESCENT   \n",
       "248967                         TYNE ST   \n",
       "248968                         TYNE ST   \n",
       "249009         MANOR FARM,SOUTH NEWTON   \n",
       "249505             TENTH LIVERPOOL ST.   \n",
       "249762                   LAUNCESTON PL   \n",
       "249874                       CLADE ST.   \n",
       "250070            CRESCENT WCEGA TOWER   \n",
       "\n",
       "                                           city    zip_code country state  \n",
       "183911                           MANAMA HAHRAIN           0     USA   NaN  \n",
       "186126                    MISS, ONTARIO, CANADA     L5N 3H5     USA   NaN  \n",
       "188915                           STOCKBURG KENT           0     USA   NaN  \n",
       "190375                                    LUCCA       55067     USA   NaN  \n",
       "191245               CHARLESTON LONDON, ENGLAND     SE 770X     USA   NaN  \n",
       "193547                Woodville, United Kingdom    Deli-7DN     USA   NaN  \n",
       "194701                            LONDON SE12ET           0     USA   NaN  \n",
       "203041                 TORONTO  ONTARIO, CANADA      M9W6C9     USA   NaN  \n",
       "203042                 TORONTO  ONTARIO, CANADA      M9W6C9     USA   NaN  \n",
       "203043                 TORONTO  ONTARIO, CANADA      M9W6C9     USA   NaN  \n",
       "216567  ST. FATIMA SQ. HELIOPOLIS, CAIRO, EGYPT       11361    Egyp   NaN  \n",
       "216568  ST. FATIMA SQ. HELIOPOLIS, CAIRO, EGYPT       11361    Egyp   NaN  \n",
       "216927                 CALGARY, ALBERTA, CANADA     T3H-4X8    Cana   NaN  \n",
       "217529                 CALGARY, ALBERTA, CANADA     T3H-4X8    Cana   NaN  \n",
       "220371                              ROBINA  QLD        4226     USA   NaN  \n",
       "220407                           UNITED KINGDOM           0     USA   NaN  \n",
       "220533                                 VICTORIA        3350     USA   NaN  \n",
       "223243                         STOCKBURY,  KENT  ME9,7QR UK     USA   NaN  \n",
       "223334                     ALFORTEVILLE, FRANCE       94140     USA   NaN  \n",
       "223335                     ALFORTEVILLE, FRANCE       94140     USA   NaN  \n",
       "224720                          CALVARY ALBERTA           0     USA   NaN  \n",
       "225113                                SINGAPORE      650213     USA   NaN  \n",
       "225794                         BET ZAIT, ISRAEL       90815     USA   NaN  \n",
       "225795                         BET ZAIT, ISRAEL       90815     USA   NaN  \n",
       "225796                         BET ZAIT, ISRAEL       90815     USA   NaN  \n",
       "227074                          QUEANBEYAN, NSW        2620     USA   NaN  \n",
       "227161                        CARLISLE CUMBRIAN          49     USA   NaN  \n",
       "227736                               NEWTONMORE           0     USA   NaN  \n",
       "227737                               NEWTONMORE           0     USA   NaN  \n",
       "227964                          CALGARY ALBERTA           0     USA   NaN  \n",
       "...                                         ...         ...     ...   ...  \n",
       "244324                               MT. BARKER        6324     USA   NaN  \n",
       "244407                        WCEGA TOWER 03-74      658065     USA   NaN  \n",
       "244547                              BELIZE CITY           0     USA   NaN  \n",
       "244660                                  BANGKOK           0     USA   NaN  \n",
       "244661                                  BONGKOK           0     USA   NaN  \n",
       "244799                          RAMOT JERUSALEM       97225     USA   NaN  \n",
       "244856                                  TORONTO         222     USA   NaN  \n",
       "244894                            N. E. CALGARY           0     USA   NaN  \n",
       "244895                                  TORONTO          22     USA   NaN  \n",
       "245023                                 CHESHIRE           0     USA   NaN  \n",
       "245024                                 CHESHIRE           0     USA   NaN  \n",
       "245302                          CALGARY,ALBERTA           0     USA   NaN  \n",
       "245316                                  CALGARY           0     USA   NaN  \n",
       "245957                                  WALTHAM         179     USA   NaN  \n",
       "245958                                  WALTHAM         179     USA   NaN  \n",
       "246218                          LODON ,N2OJE,UK           0     USA   NaN  \n",
       "247060                      GANIC TIKVA ,ISRAEL       55900     USA   NaN  \n",
       "247422                                 WATERLOO        1401     USA   NaN  \n",
       "247515                                  ONTARIO         342     USA   NaN  \n",
       "247907                          NEW SOUTH WALES        2448     USA   NaN  \n",
       "248034                                TADA CITY     3350035     USA   NaN  \n",
       "248257                              RICHMOND BC           0     USA   NaN  \n",
       "248379                                SINGAPORE      658065     USA   NaN  \n",
       "248967       BOX HILL NORTH, VICTORIA AUSTRALIA        3129     USA   NaN  \n",
       "248968       BOX HILL NORTH, VICTORIA AUSTRALIA        3129     USA   NaN  \n",
       "249009                 SALISBURY,WILT,SP2, OQD,           0     USA   NaN  \n",
       "249505                                 ROSE BAY        2029     USA   NaN  \n",
       "249762                           UNITED KINGDOM         157     USA   NaN  \n",
       "249874                             BEULAH SOUTH        5067     USA   NaN  \n",
       "250070                              BUKIT BATOK      658065     USA   NaN  \n",
       "\n",
       "[93 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check some info when state values are NaN\n",
    "train[train.state.isnull()][['mailing_address_str_name','city','zip_code','country','state']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to correct the mismatches in cities and countries, search the name of the city from google map and correct the name of the country manually. As you probably notice, there are a lot of cities not only just mismatched but also erroneously spelled e.g) TOKO, OSKA, SUREY, VAN COUVER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Below is the list of misspelled and mismatched cities.\n",
    "# While iterating over each raw, any of cities or countries in the list appears, \n",
    "# replace the erroneous labels on country column to correct labels. If there is nothing erroneous then label USA.\n",
    "def nation_conv(element):\n",
    "    \n",
    "    singapore = 'WCEGA|PLAZA|PENISULA PLAZA|PENISULA AVE|PENINSULA|PENINSALA PLAZA|SINAGAPORE|SUNGAPORE|SINGA PORE|SINGAPO|SINGAPORE|WCEGA TOWER|BUKIT BATOK|SANDY PALM|BATOK'\n",
    "    canada = 'SUN RIDGE|SURRY|RICHMOND, BC|RICHMOND|RICHMOND V7A4V3|VAN COUVER|EDMONTON|SUREY|YELLOWKNIFE|WINNIPEG MANTOBA|VANCOUVER|ALBERTA|CANADA|ONTARIO|TORONTO|RICHMOND BC|WINDSOR|CALGARY|ALDERGROVE'\n",
    "    israel = 'RAMOT|ISRAEL|RAANANA|KIRYAT MOZKIN|RAMOT JERUSALEM|RAMONT JERUSALEM'\n",
    "    uk = 'WARWICK|ROTHERHAM|PAIGNTON|ONCHAN ISLE OF MAN|NORTH PORTSMOUTH|TORTOLA|SHEFFIELD|SCARBOUROUGH|SCARBOROUGH|LANARKSHIRE|NEW CASTLE UPON TYM|MICKLEOVER|LANDONDERRY|DUNDALK|HERT FORDSHIRE|THORTON CLEVELEY\\'S|SWINDON|STAFFORDSHIRE|SOUTHERNESS|SOUTH PORT|CHINNOR ORON|CHELTENHAM|CHATHAM|BURY|CANTERBURY|CARLUKE|CHARLTON|FARNBOROUGH HANTS|CHURCH TOWN|HATFIELD|MANCHESTER|ESSEX|LANCASHIRE|WARKFIELD BERKSHIRE|WARFIELD BERKSHIRE|TRURO|BROMLEY|BRIGHT|BRIGHTON|BICKERLEY RINGWOOD|BERKSHIRE|SALISBURY|CARLISLE CUMBRIAN|UNITED KINGDOM|LONDON|ENGLAND|CHESHIRE|KENT|NEWTONMORE|UNITED KINGDO|ECKINGTON, SHEFFIELD|WISHAW|LODON ,N2OJE,UK|OJE|SALISBURY,WILT,SP2, OQD,'\n",
    "    egypt = 'NEW CIARO|EGYPT|ST FATIMA SQ HELIOP|CAIRO'\n",
    "    france = 'PARIS|FRANCE|ALQUE MORTES|MATISSE CHAMPS SUR'\n",
    "    thai = 'BANGKOK|THAILAND|BONGKOK'\n",
    "    australia = 'QUEENSLAND|SMYLHES CREEK|NEDLAND|FASSI FERN |BELLS BROOK|BULLSBRPPK|BULLSHROOK|BUNDALL|ELLES GROVE|CROYDON|CROSSOVER|COLDSTREAM|BULLSBROOK|BRISBANE OLD|BLACK TOWN|BELROSE|BASSENDEAN|AVALON CITY|ROBINA|QUEANBEYAN|NSW|AUSTRALIA|ROBINA QLD|AUSTRALI|VICTORIA|QUENSLAND|BOX HILL NORTH VICT|MT. BARKER|NEW SOUTH WALES|ROSE BAY'\n",
    "    bahrain = 'MANAMA'\n",
    "    italy = 'DI QUATIRO CA|LUCCA|ALZATE'\n",
    "    holland = 'VOORBURG|VEGHEL|NETHERLAND'\n",
    "    zealand = 'AUCKLAND|New Zealand'\n",
    "    japan = 'OSKA|KOMAE SHI|TOKYO|TOYKO|TOKO|JAPAN|CHIBA CITY'\n",
    "    china = 'TAI O|QINGXIU|QING XIU|NEW PRAYN|SHANGHAI|HONG KONG|TAIPEI|KWON TONG|KWUN TONG'\n",
    "    germany = 'WALDKRAIBURG|MUNICH|BERLIN'\n",
    "    india = 'SONEPAT HARYANA|ISLAMABAD'\n",
    "    arab = 'DUBAI'\n",
    "    malay = 'SUBANG JAYA|SALANGOR'\n",
    "    indonesia = 'SABANG JAYA'\n",
    "    lebanon = 'LEBANON'\n",
    "    nigiria = 'ALAUSA'\n",
    "    norway = 'NORWAY'\n",
    "    poland = 'POZNAN'\n",
    "    etc = 'TADA CITY|BEULAH SOUTH'\n",
    "    \n",
    "    nations = [singapore,canada,israel,uk,egypt,france,thai,australia,bahrain,italy,holland,zealand,japan,china,germany,india,arab,malay,indonesia,lebanon,nigiria,norway,poland,etc]\n",
    "    labels = ['SIN','CAN','ISR','UK','EGY','FRA','THI','AUS','BAH','ITA','HOL','NZL','JAP','CHN','GER','IND','ARA','MAL','IDN','LEB','NIG','NOR','POL','ETC']\n",
    "    \n",
    "    # If we create a dictionary, labeling would be so much easier.\n",
    "    code = dict.fromkeys([])\n",
    "    \n",
    "    for nation, label in zip(nations,labels):\n",
    "        code.update(dict.fromkeys(nation.split('|'), label))\n",
    "    \n",
    "    for nation in nations:\n",
    "        found = list(map(lambda x: x.upper(),re.findall('\\\\b' + nation + '\\\\b', element, re.IGNORECASE)))\n",
    "        if(len(found) > 0):\n",
    "            return code[found[0]]\n",
    "        continue\n",
    "    \n",
    "    return 'USA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>agency_name</th>\n",
       "      <th>inspector_name</th>\n",
       "      <th>violator_name</th>\n",
       "      <th>mailing_address_str_number</th>\n",
       "      <th>mailing_address_str_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>country</th>\n",
       "      <th>...</th>\n",
       "      <th>fine_amount</th>\n",
       "      <th>admin_fee</th>\n",
       "      <th>state_fee</th>\n",
       "      <th>late_fee</th>\n",
       "      <th>discount_amount</th>\n",
       "      <th>clean_up_cost</th>\n",
       "      <th>judgment_amount</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>num_null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12495</th>\n",
       "      <td>306086</td>\n",
       "      <td>Department of Public Works</td>\n",
       "      <td>May, Tanya</td>\n",
       "      <td>CHASE HOME FINANCE LLC, .</td>\n",
       "      <td>8</td>\n",
       "      <td>BROOKSEDGE BLVD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OH</td>\n",
       "      <td>43081</td>\n",
       "      <td>USA</td>\n",
       "      <td>...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>42.411773</td>\n",
       "      <td>-83.155902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ticket_id                 agency_name inspector_name  \\\n",
       "12495     306086  Department of Public Works     May, Tanya   \n",
       "\n",
       "                   violator_name mailing_address_str_number  \\\n",
       "12495  CHASE HOME FINANCE LLC, .                          8   \n",
       "\n",
       "      mailing_address_str_name city state zip_code country    ...     \\\n",
       "12495          BROOKSEDGE BLVD  NaN    OH    43081     USA    ...      \n",
       "\n",
       "      fine_amount admin_fee state_fee late_fee discount_amount  clean_up_cost  \\\n",
       "12495       200.0      20.0      10.0     20.0             0.0            0.0   \n",
       "\n",
       "       judgment_amount        lat        lon  num_null  \n",
       "12495            250.0  42.411773 -83.155902         1  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['city'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, there are a lot of incorrect information on country column. Consequently, it is necessary to createa new column by applying the above function 'nation_conv' for describing violator's mailing country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the function.\n",
    "train['new_country'] = train['city'].map(nation_conv)\n",
    "\n",
    "# Based on google map, the name of the city in test dataset considered as a missing value is actually Westerville,\n",
    "# which matches with the state and street name and zipcode.\n",
    "test.city.fillna('NULL', inplace=True)\n",
    "test.city.iloc[12495] = 'Westerville'\n",
    "\n",
    "# Apply the function on test set. \n",
    "test['new_country'] = test['city'].map(nation_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some name of cities, for example WATERLOO, VICTORIA, are used widely in english-speaking countries. Therefore, in order to avoid misclassification, manually label those cities by using google map. In addition, as mentioned, there are some places that cannot be found in google map and label them as ETC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Set:\n",
    "# city: WATERLOO, BELGIUM\n",
    "train['new_country'].iloc[236844] = 'BEL'\n",
    "train['new_country'].iloc[247422] = 'BEL'\n",
    "\n",
    "# city: BEVAN WAY ABBOTSFORD B.C V256R3\n",
    "train['new_country'].iloc[240635] = 'CAN'\n",
    "train['new_country'].iloc[240806] = 'CAN'\n",
    "\n",
    "# city: BELIZE CITY\n",
    "train['new_country'].iloc[244547] = 'BLZ'\n",
    "\n",
    "# Test Set:\n",
    "#city: BELIZE CITY\n",
    "test['new_country'].iloc[4328] = 'BLZ'\n",
    "\n",
    "# UNITED KINGDOM\n",
    "test['new_country'].iloc[20878] = 'UK'\n",
    "test['new_country'].iloc[5647] = 'UK'\n",
    "test['new_country'].iloc[5648] = 'UK'\n",
    "test['new_country'].iloc[17870] = 'UK'\n",
    "test['new_country'].iloc[17871] = 'UK'\n",
    "test['new_country'].iloc[16513] = 'UK'\n",
    "test['new_country'].iloc[21689] = 'UK'\n",
    "test['new_country'].iloc[22436] = 'UK'\n",
    "test['new_country'].iloc[22268] = 'UK'\n",
    "test['new_country'].iloc[17911] = 'JAP'\n",
    "\n",
    "# ETC list (failed to find the exact locations by google)\n",
    "test['new_country'].iloc[14051] = 'ETC'\n",
    "test['new_country'].iloc[14050] = 'ETC'\n",
    "test['new_country'].iloc[22103] = 'ETC'\n",
    "test['new_country'].iloc[14308] = 'ETC'\n",
    "test['new_country'].iloc[2482] = 'ETC'\n",
    "test['new_country'].iloc[18154] = 'ETC'\n",
    "test['new_country'].iloc[10523] = 'ETC'\n",
    "test['new_country'].iloc[568] = 'ETC'\n",
    "test['new_country'].iloc[8580] = 'ETC'\n",
    "test['new_country'].iloc[19819] = 'ETC'\n",
    "test['new_country'].iloc[10857] = 'ETC'\n",
    "test['new_country'].iloc[21689] = 'ETC'\n",
    "test['new_country'].iloc[16413] = 'ETC'\n",
    "test['new_country'].iloc[3087] = 'ETC'\n",
    "test['new_country'].iloc[10656] = 'ETC'\n",
    "test['new_country'].iloc[13203] = 'ETC'\n",
    "test['new_country'].iloc[12860] = 'ETC'\n",
    "test['new_country'].iloc[19478] = 'ETC'\n",
    "test['new_country'].iloc[19479] = 'ETC'\n",
    "test['new_country'].iloc[5656] = 'ETC'\n",
    "test['new_country'].iloc[10776] = 'ETC'\n",
    "test['new_country'].iloc[19061] = 'ETC'\n",
    "\n",
    "# MAYNARD RD, WALTHAM : MA \n",
    "train.state.iloc[[245957,245958]] = 'MA'\n",
    "test.state.iloc[20598] = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If mailing addresses are not domestic, change them to 'OOS' which stands for out of states.\n",
    "train.state[train['new_country'] != 'USA'] = 'OOS'\n",
    "test.state[test['new_country'] != 'USA'] = 'OOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique city names in training set: 5184\n",
      "unique city names in test set: 3266\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many unique names in city column.\n",
    "print('unique city names in training set:',len(set(train.city)))\n",
    "print('unique city names in test set:',len(set(test.city)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It appears that some values are duplicated due to extra space or upper or lower case difference even though they are\n",
    "# the same.\n",
    "train.city = train.city.apply(lambda x: x.upper().strip())\n",
    "test.city = test.city.apply(lambda x: x.upper().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique city names in training set: 4322\n",
      "unique city names in test set: 2694\n"
     ]
    }
   ],
   "source": [
    "# As you can see the results, we could greatly reduce the number of duplications.\n",
    "print('unique city names in training set:',len(set(train.city)))\n",
    "print('unique city names in test set:',len(set(test.city)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zip_code for non-us countries: 'OOS'\n",
    "train.zip_code[train.new_country != 'USA'] = 'OOS'\n",
    "test.zip_code[test.new_country != 'USA'] = 'OOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows whose state column value is NaN should be converted to string type since machine learning algorithms cannot take in None type as input. Especially in this case, we cannot not even infer any of city, state and zip_code by the given information (e.g mailing_address_str_name) label them 'NULL'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_city_index = train[train.state.isnull()].index\n",
    "train['city'].iloc[null_city_index] = 'NULL'\n",
    "train['zip_code'].iloc[null_city_index] = 'NULL'\n",
    "train['state'].iloc[null_city_index] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BL', 'NULL', 'UK', 'PR', 'QC', 'VI', 'ON', 'OOS', 'NB', 'QL', 'BC']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['BL', 'UK', 'PR', 'QC', 'VI', 'ON', 'NB', 'QL', 'BC']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there is any states other than us states in state columns.\n",
    "us_state = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "not_us_state = []\n",
    "all_state = set(train.state)|set(test.state)\n",
    "\n",
    "for a in all_state:\n",
    "    if(a not in us_state):\n",
    "        not_us_state.append(a)\n",
    "\n",
    "        \n",
    "print(not_us_state)\n",
    "\n",
    "# Make sure that exclude 'OOS' and 'NULL'\n",
    "not_us_state.remove('OOS')\n",
    "not_us_state.remove('NULL')\n",
    "\n",
    "not_us_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Below rows are wrong abbreviation for the state.\n",
    "\n",
    "# According to google map, the state of this row supposed to be Virgina VA not VI.\n",
    "# Since the rest of VI represents Virgin Island in US so remove VI from not_us_state list\n",
    "train.state.iloc[15713] = 'VA'\n",
    "not_us_state.remove('VI')\n",
    "\n",
    "# According to google map, the state of this row supposed to be Nevada NV not NB.\n",
    "train.state.iloc[181834] = 'NV'\n",
    "train.state.iloc[181835] = 'NV'\n",
    "\n",
    "# Most of states denoted as 'NB' supposed to be 'NE'; therefore, from the not_us_state list, we need to remove it \n",
    "# to avoid an error when we create the dictionary later on. NB also stands for New Brunswick which is the province in \n",
    "# Canada but decide not to manually alter since the number of rows actually New Brunswick is negligible.\n",
    "\n",
    "train.state[train.state == 'NB'] = 'NE'\n",
    "test.state[test.state == 'NB'] = 'NE'\n",
    "\n",
    "not_us_state.remove('NB')\n",
    "\n",
    "# Many rows are wrongly classified as USA. \n",
    "train.state.iloc[248498] = 'OOS'\n",
    "train.new_country.iloc[248498] = 'SIN'\n",
    "train.state.iloc[241977] = 'OOS'\n",
    "train.new_country.iloc[241977] = 'SIN'\n",
    "\n",
    "train.state.iloc[107274] = 'OOS'\n",
    "train.new_country.iloc[107274] = 'CAN'\n",
    "train.state.iloc[107275] = 'OOS'\n",
    "train.new_country.iloc[107275] = 'CAN'\n",
    "\n",
    "train.state.iloc[20801] = 'OOS'\n",
    "train.new_country.iloc[20801] = 'CHN'\n",
    "train.state.iloc[20802] = 'OOS'\n",
    "train.new_country.iloc[20802] = 'CHN'\n",
    "\n",
    "#test.state.iloc[338993] = 'OOS'\n",
    "#test.new_country.iloc[338993] = 'JAP'\n",
    "#test.state.iloc[338995] = 'OOS'\n",
    "#test.new_country.iloc[338995] = 'JAP'\n",
    "\n",
    "test.state.iloc[6550] = 'OOS'\n",
    "test.new_country.iloc[6550] = 'JAP'\n",
    "\n",
    "test.state.iloc[6551] = 'OOS'\n",
    "test.new_country.iloc[6551] = 'JAP'\n",
    "\n",
    "test.state.iloc[9266] = 'OOS'\n",
    "test.new_country.iloc[9266] = 'JAP'\n",
    "\n",
    "#test.state.iloc[357567] = 'OOS'\n",
    "#test.new_country.iloc[357567] = 'SIN'\n",
    "\n",
    "test.state.iloc[33881] = 'OOS'\n",
    "test.new_country.iloc[33881] = 'SIN'\n",
    "test.state.iloc[2736] = 'OOS'\n",
    "test.new_country.iloc[2736] = 'SIN'\n",
    "test.state.iloc[3570] = 'OOS'\n",
    "test.new_country.iloc[3570] = 'ETC'\n",
    "test.state.iloc[5427] = 'OOS'\n",
    "test.new_country.iloc[5427] = 'ETC'\n",
    "test.state.iloc[11289] = 'OOS'\n",
    "test.new_country.iloc[11289] = 'ETC'\n",
    "test.state.iloc[20175] = 'OOS'\n",
    "test.new_country.iloc[20175] = 'ETC'\n",
    "test.state.iloc[20176] = 'OOS'\n",
    "test.new_country.iloc[20176] = 'ETC'\n",
    "test.state.iloc[20234] = 'OOS'\n",
    "test.new_country.iloc[20234] = 'ETC'\n",
    "\n",
    "test.state.iloc[34188] = 'OOS'\n",
    "test.new_country.iloc[34188] = 'CAN'\n",
    "test.state.iloc[34189] = 'OOS'\n",
    "test.new_country.iloc[34189] = 'CAN'\n",
    "\n",
    "test.state.iloc[48595] = 'OOS'\n",
    "test.new_country.iloc[48595] = 'AUS'\n",
    "test.state.iloc[48596] = 'OOS'\n",
    "test.new_country.iloc[48596] = 'AUS'\n",
    "\n",
    "# The country denoted as USA but has non-us states in state column should be converted to the proper labels.\n",
    "nation_map = dict.fromkeys(['BC','ON','QC'],'CAN')\n",
    "nation_map['BL'] = 'GER'\n",
    "nation_map['UK'] = 'UK'\n",
    "nation_map['QL'] = 'AUS'\n",
    "nation_map['PR'] = 'ETC'\n",
    "\n",
    "for st in set(not_us_state):\n",
    "    train.new_country[train.state == st] = nation_map[st]\n",
    "    test.new_country[test.state == st] = nation_map[st]\n",
    "    \n",
    "    train.state[train.state == st] = 'OOS'\n",
    "    test.state[test.state == st] = 'OOS'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of states in training set: 54\n",
      "# of states in test set: 53\n"
     ]
    }
   ],
   "source": [
    "# Including NULL and OOS ('Out of States'), there should be 54 states in training set and 53 states in test set (no NULL value).\n",
    "print('# of states in training set:',len(set(train.state)))\n",
    "print('# of states in test set:',len(set(test.state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some rows whose city column has ST THOMAS are classified as VA which supposed to be VI (Virgin Irland)\n",
    "va_island = train[train.state == 'VA'].city.str.extractall(r'(.* THOMAS)').index.levels[0]\n",
    "train.state.iloc[va_island] = 'VI'\n",
    "\n",
    "va_island_test = test[test.state == 'VA'].city.str.extractall(r'(.* THOMAS)').index.levels[0]\n",
    "test.state.iloc[va_island_test] = 'VI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.15 Zip Code\n",
    "\n",
    "Zipcode column comprises string and integer; therefore, first thing to do is to convert all the zipcode to string. Also, simplify the international zipcodes as OOS 'Out Of States'. If there is invalid place cannot be found even in google map, then label them as 'NULL'. In addition, we could find many invalid zipcode which consist of three digits or less. These should be also converted to 'NULL'. Lastly, remove hyphen sign which connects main zipcode (usually 5 digits) between detail zipcodes for unity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to standardized the zip_code type. (Some of them are string and some of them are integer)\n",
    "train.zip_code = train.zip_code.apply(lambda x: str(x))\n",
    "test.zip_code = test.zip_code.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zipcode for Out of states is OOS\n",
    "train.zip_code[train.state == 'OOS'] = 'OOS'\n",
    "test.zip_code[test.state == 'OOS'] = 'OOS'\n",
    "\n",
    "# US zipcode whose default value is 0 changes to NULL\n",
    "train.zip_code[((train.zip_code == '0') | (train.zip_code == 0)) & (train.new_country == 'USA')] = 'NULL'\n",
    "test.zip_code[((test.zip_code == '0') | (test.zip_code == 0)) & (test.new_country == 'USA')] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find wrong format zipcode which consists from 0 to 3 digits. We can safely assume that upto 3 digits zipcode\n",
    "# are wrong type format; even though 4 digits can be problematic as well, the case starts with 0 postal code can be\n",
    "# 5 digits only if the type is string. ex) 04513. \n",
    "train.zip_code[train.zip_code.str.match(r'^([0-9]{0,3})?$')] = 'NULL'\n",
    "test.zip_code[test.zip_code.str.match(r'^([0-9]{0,3})?$')] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unify the format of 9 digits or 8 digits postal code as non-hyphen(or non-special characters) postal code. \n",
    "# ex) 12345-6789 => 123456789\n",
    "train.zip_code = train.zip_code.apply(lambda x: re.sub('[^A-Za-z0-9]','',x))\n",
    "test.zip_code = test.zip_code.apply(lambda x: re.sub('[^A-Za-z0-9]','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.16 Hearing Date\n",
    "\n",
    "The reason why I chose to treate the hearing date column as nominal rather than interval is that hearing time has only 6 unique values which can be easier to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values with xxxx-xx-xx xx:xx:xx.\n",
    "train.hearing_date.fillna('xxxx-xx-xx xx:xx:xx', inplace=True)\n",
    "test.hearing_date.fillna('xxxx-xx-xx xx:xx:xx', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split hearing dates by four: YR, MM, DD, time\n",
    "train['HR_YR'] = train.hearing_date.apply(lambda x:x.split()[0].split('-')[0])\n",
    "train['HR_MO'] = train.hearing_date.apply(lambda x:x.split()[0].split('-')[1])\n",
    "train['HR_DD'] = train.hearing_date.apply(lambda x:x.split()[0].split('-')[2])\n",
    "\n",
    "train['HR_time'] = train.hearing_date.apply(lambda x:x.split( )[1])\n",
    "\n",
    "test['HR_YR'] = test.hearing_date.apply(lambda x:x.split()[0].split('-')[0])\n",
    "test['HR_MO'] = test.hearing_date.apply(lambda x:x.split()[0].split('-')[1])\n",
    "test['HR_DD'] = test.hearing_date.apply(lambda x:x.split()[0].split('-')[2])\n",
    "\n",
    "test['HR_time'] = test.hearing_date.apply(lambda x:x.split( )[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.17 Ticket Issued Date\n",
    "\n",
    "Year, month, and day of the ticket issued date are treated as hearing date column except for the time part. Unlike the hearing time, it should be converted to minute basis to treat as interval type since it increases every 5 minutes, which can be too many if we create binary columns by pd.get_dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split ticket issued date by four: YR, MM, DD, time\n",
    "train['TI_YR'] = train.ticket_issued_date.apply(lambda x:x.split()[0].split('-')[0])\n",
    "train['TI_MO'] = train.ticket_issued_date.apply(lambda x:x.split()[0].split('-')[1])\n",
    "train['TI_DD'] = train.ticket_issued_date.apply(lambda x:x.split()[0].split('-')[2])\n",
    "\n",
    "train['TI_MI'] = train.ticket_issued_date.apply(lambda x:int(x.split()[1].split(':')[0])*60 + int(x.split()[1].split(':')[1]))\n",
    "\n",
    "test['TI_YR'] = test.ticket_issued_date.apply(lambda x:x.split()[0].split('-')[0])\n",
    "test['TI_MO'] = test.ticket_issued_date.apply(lambda x:x.split()[0].split('-')[1])\n",
    "test['TI_DD'] = test.ticket_issued_date.apply(lambda x:x.split()[0].split('-')[2])\n",
    "\n",
    "test['TI_MI'] = test.ticket_issued_date.apply(lambda x:int(x.split()[1].split(':')[0])*60 + int(x.split()[1].split(':')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.18 Violation Code / Violation Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>violation_code</th>\n",
       "      <th>violation_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9-1-36(a)</td>\n",
       "      <td>Failure of owner to obtain certificate of comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61-63.0600</td>\n",
       "      <td>Failed To Secure Permit For Lawful Use Of Buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9-1-36(a)</td>\n",
       "      <td>Failure of owner to obtain certificate of comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9-1-36(a)</td>\n",
       "      <td>Failure of owner to obtain certificate of comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9-1-36(a)</td>\n",
       "      <td>Failure of owner to obtain certificate of comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  violation_code                              violation_description\n",
       "0      9-1-36(a)  Failure of owner to obtain certificate of comp...\n",
       "1     61-63.0600  Failed To Secure Permit For Lawful Use Of Buil...\n",
       "2      9-1-36(a)  Failure of owner to obtain certificate of comp...\n",
       "3      9-1-36(a)  Failure of owner to obtain certificate of comp...\n",
       "4      9-1-36(a)  Failure of owner to obtain certificate of comp..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['violation_code','violation_description']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see some duplicated values, violation codes and descriptions are corresponding each other, which means we can eliminate violation_description column at the end of the preprocessing. (e.g 9-1-36(a) -> Failure of owner to obtain certificate of comp...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Even though we drop violation_description column, for the later purpose, let's appropriate the two columns.\n",
    "code_description = train[['violation_code','violation_description']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.19 Fine Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>violation_code</th>\n",
       "      <th>violation_description</th>\n",
       "      <th>disposition</th>\n",
       "      <th>fine_amount</th>\n",
       "      <th>admin_fee</th>\n",
       "      <th>state_fee</th>\n",
       "      <th>late_fee</th>\n",
       "      <th>discount_amount</th>\n",
       "      <th>clean_up_cost</th>\n",
       "      <th>judgment_amount</th>\n",
       "      <th>...</th>\n",
       "      <th>num_null</th>\n",
       "      <th>new_country</th>\n",
       "      <th>HR_YR</th>\n",
       "      <th>HR_MO</th>\n",
       "      <th>HR_DD</th>\n",
       "      <th>HR_time</th>\n",
       "      <th>TI_YR</th>\n",
       "      <th>TI_MO</th>\n",
       "      <th>TI_DD</th>\n",
       "      <th>TI_MI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81440</th>\n",
       "      <td>22-2-88</td>\n",
       "      <td>Failure of owner to keep property, its sidewal...</td>\n",
       "      <td>Not responsible by Dismissal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>USA</td>\n",
       "      <td>2007</td>\n",
       "      <td>01</td>\n",
       "      <td>29</td>\n",
       "      <td>13:30:00</td>\n",
       "      <td>2007</td>\n",
       "      <td>01</td>\n",
       "      <td>03</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      violation_code                              violation_description  \\\n",
       "81440        22-2-88  Failure of owner to keep property, its sidewal...   \n",
       "\n",
       "                        disposition  fine_amount  admin_fee  state_fee  \\\n",
       "81440  Not responsible by Dismissal          NaN        0.0        0.0   \n",
       "\n",
       "       late_fee  discount_amount  clean_up_cost  judgment_amount  ...   \\\n",
       "81440       0.0              0.0            0.0              0.0  ...    \n",
       "\n",
       "       num_null  new_country  HR_YR  HR_MO HR_DD   HR_time TI_YR TI_MO TI_DD  \\\n",
       "81440         2          USA   2007     01    29  13:30:00  2007    01    03   \n",
       "\n",
       "      TI_MI  \n",
       "81440   795  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see below, there is one missing value in training fine amount column (no missing value in test set).\n",
    "train[train.fine_amount.isnull()].iloc[:,12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({200.0: 1729, 3500.0: 1100, 1000.0: 875, 500.0: 722, 2500.0: 503, 10000.0: 59, 5000.0: 30, 345.0: 2, nan: 1})\n"
     ]
    }
   ],
   "source": [
    "# Although I searched rows of the fine amount that shares the same violation code and disposition \n",
    "# in order to fill the missing value with reasonable amount, there is no common value. \n",
    "print(Counter(train[(train.violation_code == '22-2-88') & (train.disposition == 'Not responsible by Dismissal')].iloc[:,15:].fine_amount))\n",
    "\n",
    "# Therefore, decided to use the most frequent fine amount 200.0 to fill the missing value.\n",
    "train.fine_amount.fillna(200.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.20 Redundant Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ticket_id',\n",
       " 'agency_name',\n",
       " 'inspector_name',\n",
       " 'violator_name',\n",
       " 'mailing_address_str_number',\n",
       " 'mailing_address_str_name',\n",
       " 'city',\n",
       " 'state',\n",
       " 'zip_code',\n",
       " 'country',\n",
       " 'ticket_issued_date',\n",
       " 'hearing_date',\n",
       " 'violation_code',\n",
       " 'violation_description',\n",
       " 'disposition',\n",
       " 'fine_amount',\n",
       " 'admin_fee',\n",
       " 'state_fee',\n",
       " 'late_fee',\n",
       " 'discount_amount',\n",
       " 'clean_up_cost',\n",
       " 'judgment_amount',\n",
       " 'compliance',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'num_null',\n",
       " 'new_country',\n",
       " 'HR_YR',\n",
       " 'HR_MO',\n",
       " 'HR_DD',\n",
       " 'HR_time',\n",
       " 'TI_YR',\n",
       " 'TI_MO',\n",
       " 'TI_DD',\n",
       " 'TI_MI']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at what columns are left.\n",
    "list(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redundant = ['violator_name','mailing_address_str_number','mailing_address_str_name','city','country','violation_description','num_null','hearing_date','ticket_issued_date']\n",
    "train.drop(redundant, inplace = True, axis = 1)\n",
    "test.drop(redundant, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__hearing_date, ticket_issued_date__: Since we already split the time of these two columns in detail so they are not needed.<br>\n",
    "__num_null__: As mentioned, this column is created for the convenience to check how many missing values in each row for preprocessing. For the later process, it is not needed.<br>\n",
    "__mailing_address_str_name, mailing_address_str_number, violator_name, city__: There are too many unique values and too many typos to use them as machine learning algorithms' input.<br>\n",
    "__country__: Many values are wronly classified, for example, the label supposed to be USA but Canada or vice versa.<br>\n",
    "__violation_description__: It is tanamount to violation_code, just violation_description is described as english and violation_code is encoded; however, violation_code is much easier to display so decided to remove 'violation_description' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.21 Compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, our task is to predict whether a given blight ticket will be paid on time. Therefore, all we want to know is to 'will be paid on time' or not. Label 1 if the ticket will be paid on time. Otherwise, label 0 (Fill missing values with 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.compliance.fillna(0.0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Also for modeling purpose, split the column from training set.\n",
    "y = train.compliance.astype(int)\n",
    "train.drop('compliance',axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.22 Ticket ID\n",
    "\n",
    "Ticket ID for test is important for the final result. It will be used for index of our final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_id = test.ticket_id\n",
    "test.drop('ticket_id',axis=1, inplace=True)\n",
    "\n",
    "train_id = train.ticket_id\n",
    "train.drop('ticket_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.23 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.231 Nominal Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_name</th>\n",
       "      <th>inspector_name</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>violation_code</th>\n",
       "      <th>disposition</th>\n",
       "      <th>fine_amount</th>\n",
       "      <th>admin_fee</th>\n",
       "      <th>state_fee</th>\n",
       "      <th>late_fee</th>\n",
       "      <th>...</th>\n",
       "      <th>lon</th>\n",
       "      <th>new_country</th>\n",
       "      <th>HR_YR</th>\n",
       "      <th>HR_MO</th>\n",
       "      <th>HR_DD</th>\n",
       "      <th>HR_time</th>\n",
       "      <th>TI_YR</th>\n",
       "      <th>TI_MO</th>\n",
       "      <th>TI_DD</th>\n",
       "      <th>TI_MI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>Sims, Martinzie</td>\n",
       "      <td>IL</td>\n",
       "      <td>60606</td>\n",
       "      <td>9-1-36(a)</td>\n",
       "      <td>Responsible by Default</td>\n",
       "      <td>250.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-83.124268</td>\n",
       "      <td>USA</td>\n",
       "      <td>2005</td>\n",
       "      <td>03</td>\n",
       "      <td>21</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>2004</td>\n",
       "      <td>03</td>\n",
       "      <td>16</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>Williams, Darrin</td>\n",
       "      <td>MI</td>\n",
       "      <td>48208</td>\n",
       "      <td>61-63.0600</td>\n",
       "      <td>Responsible by Determination</td>\n",
       "      <td>750.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-83.135118</td>\n",
       "      <td>USA</td>\n",
       "      <td>2005</td>\n",
       "      <td>05</td>\n",
       "      <td>06</td>\n",
       "      <td>13:30:00</td>\n",
       "      <td>2004</td>\n",
       "      <td>04</td>\n",
       "      <td>23</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>Sims, Martinzie</td>\n",
       "      <td>MI</td>\n",
       "      <td>48223</td>\n",
       "      <td>9-1-36(a)</td>\n",
       "      <td>Not responsible by Dismissal</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-83.096069</td>\n",
       "      <td>USA</td>\n",
       "      <td>2005</td>\n",
       "      <td>03</td>\n",
       "      <td>29</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>2004</td>\n",
       "      <td>04</td>\n",
       "      <td>26</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>Sims, Martinzie</td>\n",
       "      <td>MI</td>\n",
       "      <td>48214</td>\n",
       "      <td>9-1-36(a)</td>\n",
       "      <td>Not responsible by City Dismissal</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-83.095919</td>\n",
       "      <td>USA</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>xx</td>\n",
       "      <td>xx</td>\n",
       "      <td>xx:xx:xx</td>\n",
       "      <td>2004</td>\n",
       "      <td>04</td>\n",
       "      <td>26</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buildings, Safety Engineering &amp; Env Department</td>\n",
       "      <td>Sims, Martinzie</td>\n",
       "      <td>MI</td>\n",
       "      <td>48206</td>\n",
       "      <td>9-1-36(a)</td>\n",
       "      <td>Not responsible by Dismissal</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-83.208233</td>\n",
       "      <td>USA</td>\n",
       "      <td>2005</td>\n",
       "      <td>03</td>\n",
       "      <td>29</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>2004</td>\n",
       "      <td>04</td>\n",
       "      <td>26</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      agency_name    inspector_name state  \\\n",
       "0  Buildings, Safety Engineering & Env Department   Sims, Martinzie    IL   \n",
       "1  Buildings, Safety Engineering & Env Department  Williams, Darrin    MI   \n",
       "2  Buildings, Safety Engineering & Env Department   Sims, Martinzie    MI   \n",
       "3  Buildings, Safety Engineering & Env Department   Sims, Martinzie    MI   \n",
       "4  Buildings, Safety Engineering & Env Department   Sims, Martinzie    MI   \n",
       "\n",
       "  zip_code violation_code                        disposition  fine_amount  \\\n",
       "0    60606      9-1-36(a)             Responsible by Default        250.0   \n",
       "1    48208     61-63.0600       Responsible by Determination        750.0   \n",
       "2    48223      9-1-36(a)       Not responsible by Dismissal        250.0   \n",
       "3    48214      9-1-36(a)  Not responsible by City Dismissal        250.0   \n",
       "4    48206      9-1-36(a)       Not responsible by Dismissal        250.0   \n",
       "\n",
       "   admin_fee  state_fee  late_fee  ...         lon  new_country  HR_YR  HR_MO  \\\n",
       "0       20.0       10.0      25.0  ...  -83.124268          USA   2005     03   \n",
       "1       20.0       10.0      75.0  ...  -83.135118          USA   2005     05   \n",
       "2        0.0        0.0       0.0  ...  -83.096069          USA   2005     03   \n",
       "3        0.0        0.0       0.0  ...  -83.095919          USA   xxxx     xx   \n",
       "4        0.0        0.0       0.0  ...  -83.208233          USA   2005     03   \n",
       "\n",
       "   HR_DD   HR_time TI_YR TI_MO TI_DD TI_MI  \n",
       "0     21  10:30:00  2004    03    16   700  \n",
       "1     06  13:30:00  2004    04    23   750  \n",
       "2     29  10:30:00  2004    04    26   820  \n",
       "3     xx  xx:xx:xx  2004    04    26   810  \n",
       "4     29  10:30:00  2004    04    26   780  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ordinal_list = list(train.dtypes[train.dtypes=='object'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agency_name', 'inspector_name', 'state', 'zip_code', 'violation_code', 'disposition', 'new_country', 'HR_YR', 'HR_MO', 'HR_DD', 'HR_time', 'TI_YR', 'TI_MO', 'TI_DD']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 173, 54, 4111, 235, 9, 18, 13, 13, 32, 6, 11, 12, 31]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ordinal_list)\n",
    "list(map(lambda x: len(set(train[x])),ordinal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.232 Number of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above is the length of each ordinal variable's unique value. Among many numbers, 4112 is noteceable which comes from 'zip_code' column. In order to work ordinal variables in machine learning algorithms, they must be binarized; however, in the process of binarization, the number of columns increases as the number of unique values in the original column, which can cause increasing variance of the model. This leads to overfitting.<br>\n",
    "As a rule of thumb for a number of features, if we assume that variables are theoretically (perfectly) not correlated each other, N (number of samples) - 1 is allowable. If variables are perfectly correlated, N^(1/2) is the desirable number of features. If we follow this logic, as mentioned, zipcode can be potential threat to overfitting. Besides, 'zip_code' is not the only variable to represent geographical information but 'state' column (mailing_state) can represent where the violator's mailing address is. Therefore, I decide to remove 'zip_code' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.drop('zip_code',axis=1, inplace=True)\n",
    "test.drop('zip_code',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinal_list.remove('zip_code')\n",
    "sum(list(map(lambda x: len(set(train[x])), ordinal_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we binarize the nominal variables, check the types of each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agency_name         object\n",
       "inspector_name      object\n",
       "state               object\n",
       "violation_code      object\n",
       "disposition         object\n",
       "fine_amount        float64\n",
       "admin_fee          float64\n",
       "state_fee          float64\n",
       "late_fee           float64\n",
       "discount_amount    float64\n",
       "clean_up_cost      float64\n",
       "judgment_amount    float64\n",
       "lat                float64\n",
       "lon                float64\n",
       "new_country         object\n",
       "HR_YR               object\n",
       "HR_MO               object\n",
       "HR_DD               object\n",
       "HR_time             object\n",
       "TI_YR               object\n",
       "TI_MO               object\n",
       "TI_DD               object\n",
       "TI_MI                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if two datasets have same columns.\n",
    "train.columns == test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agency_name        True\n",
       "inspector_name     True\n",
       "state              True\n",
       "violation_code     True\n",
       "disposition        True\n",
       "fine_amount        True\n",
       "admin_fee          True\n",
       "state_fee          True\n",
       "late_fee           True\n",
       "discount_amount    True\n",
       "clean_up_cost      True\n",
       "judgment_amount    True\n",
       "lat                True\n",
       "lon                True\n",
       "new_country        True\n",
       "HR_YR              True\n",
       "HR_MO              True\n",
       "HR_DD              True\n",
       "HR_time            True\n",
       "TI_YR              True\n",
       "TI_MO              True\n",
       "TI_DD              True\n",
       "TI_MI              True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check types as well.\n",
    "test.dtypes == train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.233 Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__ Make sure that features are represented in the same way in the training and test set before we apply machine learning algorithms__; if training and test sets have different numbers of features (or even if the numbers are the same, they must be equal columns) and we can't apply the model we learned on the training set to the test set anymore. Therefore, we need to concatnenate before using get_dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "border = len(train)\n",
    "comb = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert object type columns to dummies.\n",
    "df_dummy = pd.get_dummies(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check if there is any missing values.\n",
    "df_dummy.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For modeling, split train and test as before.\n",
    "X_train = df_dummy.iloc[:border,:]\n",
    "X_test = df_dummy.iloc[border:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dim: (250306, 717) , Test set dim: (61001, 717)\n"
     ]
    }
   ],
   "source": [
    "print('Training set dim:',X_train.shape, ',', 'Test set dim:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "select = SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=1), threshold='median')\n",
    "select.fit(X_train,y)\n",
    "X_train_sel = select.transform(X_train)\n",
    "X_test_sel = select.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dim: (250306, 359) , Test set dim: (61001, 359)\n"
     ]
    }
   ],
   "source": [
    "print('Training set dim:',X_train_sel.shape, ',', 'Test set dim:', X_test_sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 717\n",
      "selected features' index:\n",
      " [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  23  25  26\n",
      "  28  29  32  33  36  38  39  40  42  43  50  51  59  60  61  62  64  66\n",
      "  67  71  73  76  79  80  83  84  85  87  88  89  90  93  94  98  99 103\n",
      " 104 105 107 110 111 114 117 120 121 123 125 126 128 130 131 134 135 137\n",
      " 139 141 143 144 145 146 147 148 154 155 157 158 159 160 161 162 164 165\n",
      " 172 173 176 180 181 182 184 185 186 188 189 191 192 194 195 196 197 198\n",
      " 200 201 207 208 209 210 212 213 217 219 220 221 225 226 230 231 234 235\n",
      " 236 242 243 245 246 247 248 249 250 251 253 254 255 256 258 259 260 261\n",
      " 262 263 264 265 266 267 268 271 273 274 275 278 279 280 281 282 283 284\n",
      " 285 286 288 289 290 291 294 295 297 299 300 302 305 307 309 310 315 316\n",
      " 317 318 321 325 326 327 334 336 337 338 339 340 343 357 358 359 402 408\n",
      " 409 411 412 414 415 416 420 441 444 445 446 448 451 453 459 460 461 464\n",
      " 465 468 475 486 490 499 500 501 508 509 510 543 548 553 554 555 556 558\n",
      " 561 562 563 565 567 568 571 573 580 583 591 592 593 594 595 596 597 598\n",
      " 599 600 601 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620\n",
      " 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638\n",
      " 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656\n",
      " 657 660 661 662 663 664 665 666 667 668 674 675 676 677 678 679 680 681\n",
      " 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699\n",
      " 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716]\n"
     ]
    }
   ],
   "source": [
    "# By the function get_support(), we can actually see which features are chosen.\n",
    "# We can use the variable mask later.\n",
    "mask = select.get_support()\n",
    "print('total features:',len(mask))\n",
    "print('selected features\\' index:\\n',np.where(mask == True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  250.,    20.,    10., ...,     0.,     0.,     0.],\n",
       "       [  750.,    20.,    10., ...,     0.,     0.,     0.],\n",
       "       [  250.,     0.,     0., ...,     0.,     0.,     0.],\n",
       "       ..., \n",
       "       [ 1000.,     0.,     0., ...,     0.,     0.,     0.],\n",
       "       [ 1000.,     0.,     0., ...,     0.,     0.,     0.],\n",
       "       [ 1000.,     0.,     0., ...,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set %: 95.36687094995725 test set %: 4.633129050042747\n"
     ]
    }
   ],
   "source": [
    "cc = Counter(y)\n",
    "print('training set %:',cc[0]/len(train)*100, 'test set %:',cc[1]/len(train)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAE/CAYAAABRpFldAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116d9d780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "labels, values = zip(*Counter(y).items())\n",
    "indexes = np.arange(len(labels))\n",
    "width = 0.8\n",
    "plt.bar(indexes, values,width,color=['grey','r'])\n",
    "plt.xticks(indexes + width*.045,['non-compliance','compliance'])\n",
    "plt.title('Target Variable')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph and the Counter function are showing that the data is highly imbalanced. Most of data is 0, which means as a measure of accuracy, auc_roc method is more encouraged to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Correlation\n",
    "\n",
    "In statistics, the correlation coefficient measures the strength and direction of a linear relationship between two variables. The value is always between -1 and +1. Correlation implies how two variables (X and y) are correlated but not the causation. Pandas Correlation function only displays the data whose type is non-object so the dimensionality of the correlation table will be 11 X 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-a4724a2957a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'yellow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    205\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                        copy=copy)\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    406\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   4830\u001b[0m     blocks = [make_block(\n\u001b[1;32m   4831\u001b[0m         \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4832\u001b[0;31m         placement=placement) for placement, join_units in concat_plan]\n\u001b[0m\u001b[1;32m   4833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4834\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4830\u001b[0m     blocks = [make_block(\n\u001b[1;32m   4831\u001b[0m         \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4832\u001b[0;31m         placement=placement) for placement, join_units in concat_plan]\n\u001b[0m\u001b[1;32m   4833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4834\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   4937\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m   4938\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m-> 4939\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m   4940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4941\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4937\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m   4938\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m-> 4939\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m   4940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4941\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m   5239\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5240\u001b[0m                 values = algos.take_nd(values, indexer, axis=ax,\n\u001b[0;32m-> 5241\u001b[0;31m                                        fill_value=fill_value)\n\u001b[0m\u001b[1;32m   5242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1465\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11697b240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "temp = pd.concat([comb.iloc[:border,:],y],axis=1)\n",
    "sns.heatmap(temp.corr(), annot = True, linewidth=3, linecolor='yellow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among many features, admin_fee and state_fee are noticeable; therefore we will be looking at these two more in depth later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 PairPlot\n",
    "\n",
    "For plotting, object type features should be excluded from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(temp[temp.dtypes[temp.dtypes != 'object'].index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight, the plots capture linearly separable patterns instead of completely non-linear random. Most of pairplots are either vertical line or horizontal line ,which represents the fact that the data is highly imbalanced or concentrated around specific area. Especially, the relationship between independent variables and dependent variables is the first thing to look closely. All of the graphs are showing two vertical lines (or even a dot) located at the end and start of x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 State (mailing address state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot('state',data=comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Michigan outnumbers other states, which is obvious since the project is about the city of Detroit. Therefore, I decide to visualize without Michigan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_MI = comb[comb.state != 'MI']\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot('state',data = temp_MI);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except Michigan, the top three states are California, Texas, and Florida which exactly matches with the most populous U.S states data from United States Census. Instead of interpreting the result that people from these top states are likely to commit the blight, we can roughly analyze that the population size of the states plays important role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Violation Code\n",
    "\n",
    "For visualization purpose, violation code is converted to numbers by using LabelEncoder to avoid ticks overlapped each other on the graph, which is hard to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder().fit(train.violation_code)\n",
    "vc = pd.Series(le.transform(train.violation_code))\n",
    "df_vc = pd.concat([vc,y], axis=1)\n",
    "df_vc.columns = ['code','compliance']\n",
    "df_vc.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count how many tickets were complied on time.\n",
    "df_code = df_vc.groupby('code').agg('sum')\n",
    "\n",
    "# Add a new column how many time codes occurred.\n",
    "df_code['count'] = df_vc.groupby('code').count()\n",
    "\n",
    "# Create a new column by dividing count column and compliance, which is the compliance rate.\n",
    "df_code['rate'] = df_code['compliance']/df_code['count']\n",
    "\n",
    "# Sort the dataframe by 'count' column.\n",
    "df_code = df_code.sort_values('count',ascending=False)\n",
    "\n",
    "# Remove the case violation code occurred less than one hundred times.\n",
    "df_code = df_code[df_code['count'] > 100]\n",
    "\n",
    "# Select only top 20 count values.\n",
    "df_code = df_code.iloc[:20,:]\n",
    "\n",
    "# Create the column for actual violation code.\n",
    "df_code['actual'] = list(le.inverse_transform(df_code.index))\n",
    "\n",
    "# Create the column for violation code description.\n",
    "code_description.index = code_description.violation_code\n",
    "code_description.drop('violation_code',inplace=True)\n",
    "df_code['code'] = df_code.index\n",
    "df_code = df_code.merge(code_description, left_on = 'actual',right_on='violation_code',how='inner')\n",
    "df_code.index = df_code['code']\n",
    "df_code.drop(['actual','code'],inplace=True,axis=1)\n",
    "df_code.drop_duplicates('violation_code',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_code[['compliance','count']].plot(kind='bar',stacked=True,colors=['red','grey'],figsize=(20,10));\n",
    "ax = plt.gca();\n",
    "ax.tick_params(axis='both', labelsize=20);\n",
    "plt.legend(prop={'size':20});\n",
    "plt.title('Violation Code', fontsize=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The code '9-1-43(a) - (Dwelling)' ('Failure of owner to obtain certificate of compliance') has the worst compliance rate 0.010878 and the code 22-2-61 ('Failing to secure City or Private solid waste collection containers and services') has the best compliance rate 0.120192. The best one's rate is approximately eleven times bigger than the worst one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Administrative Fee & State Fee\n",
    "\n",
    "Administrative Fee and State Fee are the most highly correlated non-object variables with target variable 0.17 respectively. Besides, we can assume that the fees how much violators have to pay may play an important role in prediction. Consequently it is worth analyzing the relationship between these variables and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(train.admin_fee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(train.state_fee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two cases on each variable: \\$0 or \\$20 for administrative fee or \\$0 or \\$10 for state fee. Maybe it is better to extract the columns from training set and combine them as separate dataframe. Also, in order to see if there is any interaction among these columns, create a new column by adding admin_fee and state_fee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fee = pd.concat([train[['admin_fee','state_fee']],y],axis=1)\n",
    "fee['summ'] = fee.admin_fee + fee.state_fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(Counter(fee.compliance))\n",
    "print('Not Paid on time:',list(Counter(fee.compliance).values())[0]/len(fee))\n",
    "print('Paid on time:',list(Counter(fee.compliance).values())[1]/len(fee))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there is any pattern when the violator paid on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fee[fee['compliance']==1].summ.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(fee[fee['compliance']==1].summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern found is that all violators who paid on time were charged for both administrative and state fee. Now let's take a look at the other case: the violators who did not pay on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(Counter(fee[fee['compliance'] == 0].summ))\n",
    "print('Not paid on time & Charged:',list(Counter(fee[fee['compliance'] == 0].summ).values())[0]/len(fee[fee['compliance'] == 0]))\n",
    "print('Not paid on time & No Charged at all:',list(Counter(fee[fee['compliance'] == 0].summ).values())[1]/len(fee[fee['compliance'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without visualization, the relationship between the violators who paid on time and fees were evident. Therefore, it is worth focusing on violators not paid on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fee[fee['compliance']==0].groupby('summ').agg('count')['compliance'].plot(kind='bar',color=[plt.cm.Paired(np.arange(len([0,30])))],figsize=(10,8));\n",
    "ax = plt.gca();\n",
    "plt.title('Not paid on time',fontsize=20);\n",
    "plt.xticks(np.arange(2),['No Charged','Charged'],rotation=0,fontsize=15);\n",
    "plt.ylabel('count',fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, there are some important takeaways. First as mentioned, all the violators who paid on time had been charged both administrative and state fees \\$30. The violators who did not pay on time consist of two groups: Not charged any of these fees or fully charged as the groups paid on time. However, the charged group is approximately twice bigger than the group with no charge. We may be able to gain a better insight by considering other variables which engender the ratio difference. ex. maybe the violation code or explanation code can help explaining such a difference between the group with charge and no charge in the violators who did not pay on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is highly imbalanced, it is important to keep the ratio of two classes in each cross validation split. Among many cross-validation methodologies, StratifiedKFold is the first thing we need to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(10, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "250,000 samples are definitely not small sized dataset and due to the limit of my laptop's CPU and memory, I decide to use Logistic Regression. Logistic Regression is a simple well-behaved, as the problem is linearly separable. Also it is pretty robust to noise with proper regularization (L1/L2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1)\n",
    "\n",
    "param = {\n",
    "            'C': [0.001],\n",
    "            'penalty':['l1'],\n",
    "            'class_weight':[{1:2}]\n",
    "        }\n",
    "\n",
    "lrCV = GridSearchCV(lr, param_grid=param, cv=kfold, verbose=True, n_jobs=-1, scoring='roc_auc')\n",
    "lrCV.fit(X_train_sel,y)\n",
    "lrCV_best = lrCV.best_estimator_\n",
    "print(lrCV.best_params_)\n",
    "\n",
    "# CV = 10 (ORIGINAL WITHOUT INSPECTOR NAME) - preserved in original file.\n",
    "# * {'C': 0.001, 'class_weight': {1: 2}, 'penalty': 'l1'} - 0.8642844\n",
    "\n",
    "# CV = 10 (SELECTFROMMODEL)\n",
    "# * {'C': 0.001, 'class_weight': {1: 2}, 'penalty': 'l1'} - 0.86427307750706317\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_log = pd.DataFrame(lrCV.predict(X_test_sel), index = test_id)\n",
    "result_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To check, we can count how many 0 and how many 1 are predicted\n",
    "result_summary = result_log.apply(pd.value_counts)\n",
    "result_summary['%'] = result_summary.apply(lambda x:x/np.sum(result_summary[0])*100)\n",
    "result_summary.columns = ['count','%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This 0 and 1 ratio might be promising and optimistic signal since the ratio and that of original dataset are very alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sv = LinearSVC(random_state=2018)\n",
    "\n",
    "param_grid = {\n",
    "    'loss':['squared_hinge'],\n",
    "    'class_weight':[{1:1}],\n",
    "    'C': [0.001],\n",
    "    'penalty':['l1'],\n",
    "    'dual':[False]\n",
    "}\n",
    "\n",
    "gs_sv = GridSearchCV(sv, param_grid = param_grid, verbose = 1, cv = kfold, n_jobs = 1, scoring = \"roc_auc\")\n",
    "gs_sv.fit(X_train_sel,y)\n",
    "gs_sv_best = gs_sv.best_estimator_\n",
    "print(gs_sv.best_params_)\n",
    "\n",
    "# L2 + Squared Hinge (default) + dual True\n",
    "# {'C': 0.03, 'class_weight': {1: 1}, 'penalty': 'l2'} - 0.8221119743329911\n",
    "\n",
    "# L2 + Hinge + dual True\n",
    "# {'C': 0.001, 'class_weight': 'balanced', 'loss': 'hinge', 'penalty': 'l2'} - 0.83519746888834068 (15%)\n",
    "\n",
    "# L1 + Squared Hinge + dual False - 0.8585770577014018 (1.17%)\n",
    "# {'C': 0.001, 'class_weight': {1: 1}, 'dual': False, 'loss': 'squared_hinge', 'penalty': 'l1'}\n",
    "\n",
    "# L2 + Squared Hinge + dual False - 0.82509376943573753\n",
    "# {'C': 0.001, 'class_weight': {1: 2}, 'dual': False, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_sv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_svm = pd.DataFrame(gs_sv.predict(X_test_sel), index = test_id)\n",
    "result_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To check, we can count how many 0 and how many 1 are predicted\n",
    "result_summary = result_svm.apply(pd.value_counts)\n",
    "result_summary['%'] = result_summary.apply(lambda x:x/np.sum(result_summary[0])*100)\n",
    "result_summary.columns = ['count','%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Bernoulli Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bnb = BernoulliNB()\n",
    "gs_bnb = GridSearchCV(bnb, param_grid = {'alpha': [0.001],\n",
    "                                         'binarize': [1]}, verbose = 1, cv = kfold, n_jobs = 1, scoring = \"roc_auc\")\n",
    "gs_bnb.fit(X_train_sel,y)\n",
    "gs_bnb_best = gs_bnb.best_estimator_\n",
    "print(gs_bnb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_bnb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(gs_bnb.predict(X_test_sel), index = test_id)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To check, we can count how many 0 and how many 1 are predicted\n",
    "result_summary = result.apply(pd.value_counts)\n",
    "result_summary['%'] = result_summary.apply(lambda x:x/np.sum(result_summary[0])*100)\n",
    "result_summary.columns = ['count','%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Logistic Regression\n",
    "\n",
    "Although we have already done measuring the feature importance by Random Forest at the feature selection stage, this time we can try it with fully tuned Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_lr = lrCV.best_estimator_.coef_\n",
    "print('Dimension of coefficient:',coef_lr.shape)\n",
    "coef_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to L1 regularization (Lasso), many coefficient estimates shrink to 0 since the absolute values of that least squares coefficient is less than lambda/2. This fact works as a main advantage for feature selection. There are less features for analysts to concern and significantly become easier to analyze as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the number of columns.\n",
    "sel_col = X_train.loc[:,mask].columns\n",
    "print('Number of selected columns:', sel_col.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mask variable comes from the feature selection part and this is from get_support function from feature selection. What it does is represent which features are chosen for feature selection as a form of binary(True or False) in an array. With the True and False list, we can extract the name of the features from X_train so that we can see which coefficients are corresponding to which features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe \n",
    "df_imp_lr = pd.DataFrame(np.reshape(coef_lr,-1),index=sel_col)\n",
    "df_imp_lr.columns = ['importance']\n",
    "\n",
    "# Create a new column for absolute values to sort elements efficiently.\n",
    "df_imp_lr['abs'] = abs(df_imp_lr['importance'])\n",
    "\n",
    "# Exclude 0 coefficients.\n",
    "df_imp_lr = df_imp_lr[df_imp_lr['abs']>0]\n",
    "\n",
    "# Sort\n",
    "df_imp_lr.sort_values('abs',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.barplot(x=df_imp_lr['abs'], y=df_imp_lr.index);\n",
    "plt.title('Feature Importance sorted by absolute coefficients', fontsize=20);\n",
    "ax = plt.gca();\n",
    "plt.yticks(fontsize=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef = gs_sv.best_estimator_.coef_\n",
    "print('Dimension of coefficient:',coef.shape)\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the number of columns.\n",
    "sel_col = X_train.loc[:,mask].columns\n",
    "print('Number of selected columns:', sel_col.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe \n",
    "df_imp = pd.DataFrame(np.reshape(coef,-1),index=sel_col)\n",
    "df_imp.columns = ['importance']\n",
    "\n",
    "# Create a new column for absolute values to sort elements efficiently.\n",
    "df_imp['abs'] = abs(df_imp['importance'])\n",
    "\n",
    "# Exclude 0 coefficients.\n",
    "df_imp = df_imp[df_imp['abs']>0]\n",
    "\n",
    "# Sort\n",
    "df_imp.sort_values('abs',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.barplot(x=df_imp['abs'], y=df_imp.index);\n",
    "plt.title('Feature Importance sorted by absolute coefficients', fontsize=20);\n",
    "ax = plt.gca();\n",
    "plt.yticks(fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to notice here is that the top five most important features selected by both models linearSVM and logistic regression are very similar. Especially, the top two \"disposition_Responsible by Default\" and \"disposition_Responsible by Determination\" are noticeably larger than any other features. By the fact that these two features are originally from \"disposition (judgement and judgement type)\", we can infer that among many factors, the decision made by Michigan (or Detroit city) court is very important whether the tickets will be paid on time or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8 Learning Curve\n",
    "Below is the criterion how well the models perform:<br>\n",
    "- If two curves are close to each other and both of them have a low score. The model suffers from an under-fitting problem (High Bias)\n",
    "- If training curve performs much better but validation curve has a lower score, then we can say the model suffers from an over-fitting problem (High Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lcplot(model,X,y,title,train_sizes=np.linspace(0.1,1.0,5)):\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(model,X,y,cv=kfold,n_jobs=1,train_sizes=train_sizes,scoring='roc_auc')\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores,axis=1)\n",
    "    train_scores_std = np.std(train_scores,axis=1)\n",
    "    \n",
    "    test_scores_mean = np.mean(test_scores,axis=1)\n",
    "    test_scores_std = np.std(test_scores,axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training size\")\n",
    "    plt.ylabel(\"ROC_AUC Score\")\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha = 0.1, color='r')\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha = 0.1, color='g')\n",
    "    plt.plot(train_sizes,train_scores_mean, 'o-', color='r', label='Training Score')\n",
    "    plt.plot(train_sizes,test_scores_mean, 'o-', color='g', label='CV score')\n",
    "    plt.legend(loc='best')\n",
    "    return plt\n",
    "\n",
    "g = lcplot(lrCV.best_estimator_,X_train_sel,y,\"Logistic Regression\")\n",
    "g = lcplot(gs_sv.best_estimator_,X_train_sel,y,\"Support Vector Machine\")\n",
    "g = lcplot(gs_bnb.best_estimator_,X_train_sel,y,\"Naive Bayes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Let's start with logistic regression learning curves. By the fact that the plot is showing the two curves are close to each other, we can say it is not clearly over-fitted model. Also, the validation score is not too low and at the same time the training score is not too high. As the sample size increases along x-axis, the initial segment of the validation curve is increasing as well; however, as the curve reaches the end, both curves are plateaued out. We can conclude that the model is neither over-fitted nor under-fitted. \n",
    "   For SVM learning curves, the gap between two curves indicates that the model suffers from variance (but not significantly high) and it would be improved by getting more samples (because the validation curve could converge toward the training curve if more training samples were added) or reducing more features. \n",
    "   On the other hand, Naive Bayes learning curves seem to suffer from bias, which means the model is under-fitted. As evidences, the score is not that high compared to other models and at the same time, the curves are roughly horizontal line (not increasing or decreasing). Unlike the SVM case, adding more data is very unlikely to help since the two curves have already converged. Perhaps we can fix the problem by adding more complexity by adding more features such as polynomial or any features representing interactions between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "As instructed at the beginning, the source of the dataset is from Coursera. Therefore this project is originally designed for the students who register the course Applied Machine Learning in Python held by University of Michigan, Ann Arbor. I took this course and submitted my first copy about an year ago. I was able to score 79 percent (above 75 is the passing with full credit) of accuracy (roc_auc) in test data set.\n",
    "This notebook is completely different than the notebook that I submitted an year ago and put definitely more effort to polish and used various approaches to achieve better performance.\n",
    "\n",
    "By redoing this project, I could refresh my knowledge that I have not used for a long time and reinforce important machine learning concepts that I thought that I understood.\n",
    "\n",
    "There are a few difficulties I encountered while I was working on the project. One of them was that I could not check my predictions directly with test dataset labels (y-test). \n",
    "As mentioned,  I took this course about an year ago so I cannot access to this course any more. Seemingly, Kaggle also provides the same dataset; however, the competition is open for only limited participants and blocked a submission. As a solution for this, I used a learning curve to check the validity of my models.  \n",
    "\n",
    "The other difficulty that I can think of is there are a lot of typos and erroneous data in the dataset especially geographical information such as address, state, country etc. I had to do a lot of manual work with google map to correct these errors. Luckily a lot of typos are concentrated in international mailing address e.g. Tokyo is classified as city in US or Waterloo which is very popular city name in many western countries is considered as missing values on country column or the address even does not exist in google map, etc. Also the abbreviation for US states are erroneously denoted e.g Nebraska should be denoted as NE not NB. Virginia should be VA not VI which is Virgin Island. \n",
    "\n",
    "Lastly, due to the constraint of my laptop's memory and lack of CPU power, I only considered simple models such as Logistic Regression, Linear Support Vector Machine and Naive Bayes; however, if your system is sustanable enough to run more complex models including Random Forest or Boostings, I highly recommend using and combine them to improve the performance. For feature selection, feel free to try other methods, some of which are recursive feature engineering, univariate ANOVA, manuel selection (I could score slightly higher accuracy by dropping inspector_name column instead of feature selection by Random Forest; however, I decided to stick with feature selection method here.) or even PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "nNS8l",
   "launcher_item_id": "yWWk7",
   "part_id": "w8BSS"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
